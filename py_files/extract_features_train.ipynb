{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas seaborn matplotlib sklearn\n",
    "# !pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>object_img</th>\n",
       "      <th>text_features</th>\n",
       "      <th>image_features</th>\n",
       "      <th>pred_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>520</td>\n",
       "      <td>Фотография. Г. Пермь. Здание горисполкома.  ПО...</td>\n",
       "      <td>799</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1817</td>\n",
       "      <td>Фотонегатив пленочный. Труппа театра «У моста»...</td>\n",
       "      <td>854</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188</td>\n",
       "      <td>Изразец гладкий расписной \"пермский\"- карниз А...</td>\n",
       "      <td>1794</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                        description  object_img  \\\n",
       "0   520  Фотография. Г. Пермь. Здание горисполкома.  ПО...         799   \n",
       "1  1817  Фотонегатив пленочный. Труппа театра «У моста»...         854   \n",
       "2   188  Изразец гладкий расписной \"пермский\"- карниз А...        1794   \n",
       "\n",
       "  text_features image_features  pred_id  \n",
       "0          None           None        0  \n",
       "1          None           None        0  \n",
       "2          None           None        0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_path = \"/home/docker_current/datasets/train\"\n",
    "df_train = pd.read_csv(\"/home/docker_current/datasets/train.csv\")\n",
    "df_train['text_features'] = None\n",
    "df_train['image_features'] = None\n",
    "df_train['pred_id'] = 0\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multilingual_clip import Config_MCLIP\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "class MultilingualCLIP(transformers.PreTrainedModel):\n",
    "    config_class = Config_MCLIP.MCLIPConfig\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(config.modelBase, cache_dir=kwargs.get(\"cache_dir\"))\n",
    "        self.LinearTransformation = torch.nn.Linear(in_features=config.transformerDimensions,\n",
    "                                                    out_features=config.numDims)\n",
    "\n",
    "    def forward(self, txt, tokenizer):\n",
    "        txt_tok = tokenizer(txt, padding=True, return_tensors='pt', truncation=True)\n",
    "        embs = self.transformer(**txt_tok)[0]\n",
    "        att = txt_tok['attention_mask']\n",
    "        embs = (embs * att.unsqueeze(2)).sum(dim=1) / att.sum(dim=1)[:, None]\n",
    "        return self.LinearTransformation(embs)\n",
    "\n",
    "    @classmethod\n",
    "    def _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init=True):\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model, [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_text_encode_model(model_name='M-CLIP/XLM-Roberta-Large-Vit-L-14'):\n",
    "    model = MultilingualCLIP.from_pretrained(model_name)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_image_encode_model(model_name=\"ViT-L/14\"):\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    return model, preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_model, text_tokenizer = get_text_encode_model()\n",
    "image_model, image_preproc = get_image_encode_model()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_text_features(df, index_text, index_image):\n",
    "    sample_text = df['description'][index_text]\n",
    "    sample_image_path = \"/home/docker_current/datasets/train/\" + str(df['object_img'][index_image]) + \".png\"\n",
    "    # print(sample_text)\n",
    "    # print(sample_image_path)\n",
    "\n",
    "    image = Image.open(sample_image_path)\n",
    "    image = image_preproc(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = image_model.encode_image(image).cpu().detach().numpy()\n",
    "\n",
    "    text_features = text_model.forward(sample_text, text_tokenizer).cpu().detach().numpy()\n",
    "\n",
    "    return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker_current/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/docker_current/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 32min 19s, sys: 2min 6s, total: 2h 34min 25s\n",
      "Wall time: 24min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Добавляем в датафрейм фичи текста и изображений, чтоб потом с ними быстро работать\n",
    "# for i in range(5):\n",
    "for i in range(len(df_train)):\n",
    "    image_features, text_features = get_image_text_features(df_train, i, i)\n",
    "    df_train['text_features'][i] = text_features\n",
    "    df_train['image_features'][i] = image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['image_features'] = df_train['image_features'].apply(lambda x: list(x[0]))\n",
    "df_train['text_features'] = df_train['text_features'].apply(lambda x: list(x[0]))\n",
    "# df_train.to_csv('train_MLCLIP_feat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker_current/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "/home/docker_current/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "image_feat_array = np.vstack((i for i in df_train['image_features']))\n",
    "text_feat_array = np.vstack((i for i in df_train['text_features']))\n",
    "\n",
    "with open('image_feat_array.npy', 'wb') as f:\n",
    "    np.save(f, image_feat_array)\n",
    "\n",
    "with open('text_feat_array.npy', 'wb') as f:\n",
    "    np.save(f, text_feat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 722 µs, sys: 14 µs, total: 736 µs\n",
      "Wall time: 439 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2738], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# # from tqdm.notebook import tqdm\n",
    "\n",
    "# cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "# sim = []\n",
    "\n",
    "# for index_image in range(len(df_train)):\n",
    "#     image_features, text_features = get_image_text_features(df_train, 0, index_image)\n",
    "#     similarity = cos(image_features, text_features)\n",
    "#     sim.append(similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multilingual_clip import pt_multilingual_clip\n",
    "# import transformers\n",
    "\n",
    "# texts = [\n",
    "#     'Three blind horses listening to Mozart.',\n",
    "#     'Älgen är skogens konung!',\n",
    "#     'Wie leben Eisbären in der Antarktis?',\n",
    "#     'Вы знали, что все белые медведи левши?'\n",
    "# ]\n",
    "# model_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n",
    "\n",
    "# # Load Model & Tokenizer\n",
    "# model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# embeddings = model.forward(texts, tokenizer)\n",
    "# print(\"Text features shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import clip\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# image = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.encode_image(image)\n",
    "\n",
    "# print(\"Image features shape:\", image_features.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
