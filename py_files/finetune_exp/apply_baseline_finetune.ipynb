{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/docker_current/py_files/MLCLIP_exp\")\n",
    "from MLCLIP_utils import get_text_encode_model, get_image_encode_model\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DumNet_4l(\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc4): Linear(in_features=256, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models_arc import DumNet, DumNet_4l\n",
    "# clarify_model = DumNet()\n",
    "# path_weights = \"symmetr_dataset_dummynet.pth\"\n",
    "\n",
    "clarify_model = DumNet_4l()\n",
    "path_weights = \"symmetr_dataset_dummynet_4l.pth\"\n",
    "\n",
    "clarify_model.load_state_dict(torch.load(path_weights, map_location=torch.device('cpu')) )\n",
    "clarify_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# model_name='M-CLIP/XLM-Roberta-Large-Vit-L-14' # самый первый, он же дефолтный\n",
    "# model_name = 'M-CLIP/XLM-Roberta-Large-Vit-B-32' #2.24 Gb\n",
    "# model_name = 'M-CLIP/XLM-Roberta-Large-Vit-B-16Plus' # Судя по метрикам самый лучший из данного зоопарка\n",
    "\n",
    "image_model, image_preproc = get_image_encode_model()\n",
    "text_model, text_tokenizer = get_text_encode_model()\n",
    "\n",
    "def get_image_features(key):\n",
    "    sample_image_path = \"/home/docker_current/datasets/test/\" + str(key) + \".png\"\n",
    "    image = Image.open(sample_image_path)\n",
    "    image = image_preproc(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = image_model.encode_image(image).cpu().detach().numpy()\n",
    "\n",
    "    return image_features\n",
    "\n",
    "def get_text_features(df, index_text):\n",
    "    sample_text = df['description'][index_text]\n",
    "    text_features = text_model.forward(sample_text, text_tokenizer)\n",
    "    text_features = clarify_model(text_features.to(device)).cpu().detach().numpy()\n",
    "    return text_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>text_features</th>\n",
       "      <th>object_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>486</td>\n",
       "      <td>Фотография. Елизавета Алексеевна Юманова.  ПКМ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813</td>\n",
       "      <td>Фотография. Заседание комитета комсомола мотор...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2980</td>\n",
       "      <td>Фотография. День \"Саланга\".  ПОКМ-18530/638 фо...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                        description text_features  \\\n",
       "0   486  Фотография. Елизавета Алексеевна Юманова.  ПКМ...          None   \n",
       "1   813  Фотография. Заседание комитета комсомола мотор...          None   \n",
       "2  2980  Фотография. День \"Саланга\".  ПОКМ-18530/638 фо...          None   \n",
       "\n",
       "  object_img  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#считываем датафрейм, добавляем столбик для эмбеддингов\n",
    "# test_images_path = \"/home/docker_current/datasets/test\"\n",
    "df_test = pd.read_csv(\"/home/docker_current/datasets/test.csv\")\n",
    "df_test['text_features'] = None\n",
    "df_test['object_img'] = None\n",
    "\n",
    "# считываем тестовые изображения, делаем словарь с эмбеддингами\n",
    "test_images = [i.split('.png')[0] for i in os.listdir(\"/home/docker_current/datasets/test\")]\n",
    "test_embed = {i:None for i in test_images}\n",
    "\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ce4c0e6434481f9f5b4304ea6c138a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker_current/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 27min 55s, sys: 1min 33s, total: 1h 29min 28s\n",
      "Wall time: 13min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for ind_text in tqdm(range(len(df_test))):\n",
    "    df_test['text_features'][ind_text] = get_text_features(df_test, ind_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ec8ce197242748a4b758888c05d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name_image in tqdm(test_embed.keys()):\n",
    "    test_embed[name_image] = get_image_features(name_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "copy_test_embed = deepcopy(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "def get_similarity(image_emb, text_emb):\n",
    "    sim =cos(torch.Tensor(image_emb), \n",
    "                        torch.Tensor(text_emb))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5d24e98b044df7a4ce011929fc0e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "# ind_text = 0\n",
    "for ind_text in tqdm(range(len(df_test))):\n",
    "\n",
    "    sims = []\n",
    "    for image_name in copy_test_embed.keys():\n",
    "        sim = get_similarity(copy_test_embed[image_name], \n",
    "                                    df_test['text_features'][ind_text])\n",
    "        sims.append(sim)\n",
    "\n",
    "    sims = np.array(sims)\n",
    "    ind_max = np.argmax(sims)\n",
    "    match_image = list(copy_test_embed.keys())[ind_max]\n",
    "    preds.append(match_image)\n",
    "    # copy_test_embed.pop(match_image, None)\n",
    "    del copy_test_embed[match_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('/home/docker_current/py_files/sample_solution.csv')\n",
    "submit['object_img'] = preds\n",
    "submit['object_img'] = submit['object_img'].astype(np.int64)\n",
    "submit.to_csv('symmetr_dataset_dummynet_4l.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
