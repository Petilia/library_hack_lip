{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from open_clip import ClipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573, 768)\n"
     ]
    }
   ],
   "source": [
    "with open('/home/docker_current/py_files/MLCLIP_exp/image_feat_array.npy', 'rb') as f:\n",
    "    image_feat_array = np.load(f)\n",
    "\n",
    "with open('/home/docker_current/py_files/MLCLIP_exp/text_feat_array.npy', 'rb') as f:\n",
    "    text_feat_array = np.load(f)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_feat_array, image_feat_array, \n",
    "                                                        test_size=0.25, random_state=42)\n",
    "\n",
    "you_are_stupid = False\n",
    "\n",
    "if you_are_stupid:\n",
    "    X_train, y_train  = np.vstack((X_train, y_train)), np.vstack((y_train, X_train))\n",
    "    print(\"you are stupid\")\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train) \n",
    "y_train_t = torch.FloatTensor(y_train) \n",
    "X_val_t = torch.FloatTensor(X_test) \n",
    "y_val_t = torch.FloatTensor(y_test) \n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ratio = 0.2\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = self.fc2(x)\n",
    "        x = ratio * x + (1 - ratio) * input\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ratio = 0.2\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = self.fc2(x)\n",
    "        x = ratio * x + (1 - ratio) * input\n",
    "        return x\n",
    "\n",
    "class MetaCLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encode_image = ImageEncoder()\n",
    "        self.encode_text = TextEncoder()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        #open_clip realization\n",
    "        image_features = self.encode_image(image)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        text_features = self.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        return image_features, text_features, self.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from contextlib import suppress\n",
    "\n",
    "# amp_bfloat16 is more stable than amp float16 for clip training\n",
    "def get_autocast(precision):\n",
    "    if precision == 'amp':\n",
    "        return torch.cuda.amp.autocast\n",
    "    elif precision == 'amp_bfloat16':\n",
    "        return lambda: torch.cuda.amp.autocast(dtype=torch.bfloat16)\n",
    "    else:\n",
    "        return suppress\n",
    "\n",
    "def is_global_master(args):\n",
    "    return args.rank == 0\n",
    "\n",
    "def is_local_master(args):\n",
    "    return args.local_rank == 0\n",
    "\n",
    "def is_master(args, local=False):\n",
    "    return is_local_master(args) if local else is_global_master(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assign_learning_rate(optimizer, new_lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = new_lr\n",
    "\n",
    "def _warmup_lr(base_lr, warmup_length, step):\n",
    "    return base_lr * (step + 1) / warmup_length\n",
    "\n",
    "def cosine_lr(optimizer, base_lr, warmup_length, steps):\n",
    "    def _lr_adjuster(step):\n",
    "        if step < warmup_length:\n",
    "            lr = _warmup_lr(base_lr, warmup_length, step)\n",
    "        else:\n",
    "            e = step - warmup_length\n",
    "            es = steps - warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr\n",
    "        assign_learning_rate(optimizer, lr)\n",
    "        return lr\n",
    "    return _lr_adjuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import optim\n",
    "\n",
    "class Args:\n",
    "    device = 'cuda:0'\n",
    "    precision = 'amp'\n",
    "    local_loss = False\n",
    "    gather_with_grad = False\n",
    "    rank = 0\n",
    "    world_size = 1\n",
    "    horovod = False\n",
    "    norm_gradient_clip = None\n",
    "    batch_size = 64\n",
    "    wandb = False\n",
    "    val_frequency = 5\n",
    "    save_logs = False\n",
    "    epochs = 300\n",
    "    lr = 0.2e-4\n",
    "\n",
    "class SampleData():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataloader = DataLoader(dataset, batch_size=64)\n",
    "        self.dataloader.num_samples = len(dataset)\n",
    "        self.dataloader.num_batches = len(self.dataloader)\n",
    "\n",
    "args = Args()\n",
    "\n",
    "data = {}\n",
    "data['train'] = SampleData(train_dataset, args.batch_size)\n",
    "data['val'] = SampleData(val_dataset, args.batch_size)\n",
    "\n",
    "model = MetaCLIP()\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scaler = GradScaler() if args.precision == \"amp\" else None\n",
    "\n",
    "total_steps = data[\"train\"].dataloader.num_batches * args.epochs\n",
    "scheduler = cosine_lr(optimizer, args.lr, 15, total_steps)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaCLIP(\n",
       "  (encode_image): ImageEncoder(\n",
       "    (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "    (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "  )\n",
       "  (encode_text): TextEncoder(\n",
       "    (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "    (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "from open_clip import ClipLoss\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def unwrap_model(model):\n",
    "    if hasattr(model, 'module'):\n",
    "        return model.module\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, tb_writer=None):\n",
    "    device = torch.device(args.device)\n",
    "    autocast = get_autocast(args.precision)\n",
    "\n",
    "    model.train()\n",
    "    loss = ClipLoss(\n",
    "        local_loss=args.local_loss,\n",
    "        gather_with_grad=args.gather_with_grad,\n",
    "        cache_labels=True,\n",
    "        rank=args.rank,\n",
    "        world_size=args.world_size,\n",
    "        use_horovod=args.horovod)\n",
    "\n",
    "    # data['train'].set_epoch(epoch)  # set epoch in process safe manner via sampler or shared_epoch\n",
    "    dataloader = data['train'].dataloader\n",
    "    # dataloader = data['train']\n",
    "\n",
    "    num_batches_per_epoch = dataloader.num_batches\n",
    "    sample_digits = math.ceil(math.log(dataloader.num_samples + 1, 10))\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        step = num_batches_per_epoch * epoch + i\n",
    "        scheduler(step)\n",
    "\n",
    "        images, texts = batch\n",
    "        images = images.to(device=device, non_blocking=True)\n",
    "        texts = texts.to(device=device, non_blocking=True)\n",
    "\n",
    "        data_time_m.update(time.time() - end)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            image_features, text_features, logit_scale = model(images, texts)\n",
    "            total_loss = loss(image_features, text_features, logit_scale)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(total_loss).backward()\n",
    "            if args.horovod:\n",
    "                optimizer.synchronize()\n",
    "                scaler.unscale_(optimizer)\n",
    "                if args.norm_gradient_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "                with optimizer.skip_synchronize():\n",
    "                    scaler.step(optimizer)\n",
    "            else:\n",
    "                if args.norm_gradient_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "                scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            if args.norm_gradient_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Note: we clamp to 4.6052 = ln(100), as in the original paper.\n",
    "        with torch.no_grad():\n",
    "            unwrap_model(model).logit_scale.clamp_(0, math.log(100))\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        batch_count = i + 1\n",
    "\n",
    "        # print(total_loss)\n",
    "        \n",
    "        if is_master(args) and (i % 100 == 0 or batch_count == num_batches_per_epoch):\n",
    "            batch_size = len(images)\n",
    "            num_samples = batch_count * batch_size * args.world_size\n",
    "            samples_per_epoch = dataloader.num_samples\n",
    "            percent_complete = 100.0 * batch_count / num_batches_per_epoch\n",
    "\n",
    "            # NOTE loss is coarsely sampled, just master node and per log update\n",
    "            loss_m.update(total_loss.item(), batch_size)\n",
    "            logit_scale_scalar = logit_scale.item()\n",
    "            logging.info(\n",
    "                f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                f\"Batch (t): {batch_time_m.avg:.3f}, {args.batch_size*args.world_size / batch_time_m.val:#g}/s \"\n",
    "                f\"LR: {optimizer.param_groups[0]['lr']:5f} \"\n",
    "                f\"Logit Scale: {logit_scale_scalar:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Save train loss / etc. Using non avg meter values as loggers have their own smoothing\n",
    "            log_data = {\n",
    "                \"loss\": loss_m.val,\n",
    "                \"data_time\": data_time_m.val,\n",
    "                \"batch_time\": batch_time_m.val,\n",
    "                \"samples_per_scond\": args.batch_size*args.world_size / batch_time_m.val,\n",
    "                \"scale\":  logit_scale_scalar,\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "            for name, val in log_data.items():\n",
    "                name = \"train/\" + name\n",
    "                if tb_writer is not None:\n",
    "                    tb_writer.add_scalar(name, val, step)\n",
    "                if args.wandb:\n",
    "                    assert wandb is not None, 'Please install wandb.'\n",
    "                    wandb.log({name: val, 'step': step})\n",
    "\n",
    "            # resetting batch / data time meters per log window\n",
    "            batch_time_m.reset()\n",
    "            data_time_m.reset()\n",
    "    # end for\n",
    "\n",
    "\n",
    "def evaluate(model, data, epoch, args, tb_writer=None):\n",
    "    metrics = {}\n",
    "    # if not is_master(args):\n",
    "    #     return metrics\n",
    "    device = torch.device(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # zero_shot_metrics = zero_shot_eval(model, data, epoch, args)\n",
    "    # metrics.update(zero_shot_metrics)\n",
    "\n",
    "    autocast = get_autocast(args.precision)\n",
    "\n",
    "    \n",
    "    # if 'val' in data and (args.val_frequency and ((epoch % args.val_frequency) == 0 or epoch == args.epochs)):\n",
    "    dataloader = data['val'].dataloader\n",
    "    num_samples = 0\n",
    "    samples_per_val = dataloader.num_samples\n",
    "\n",
    "    # FIXME this does not scale past small eval datasets\n",
    "    # all_image_features @ all_text_features will blow up memory and compute very quickly\n",
    "    cumulative_loss = 0.0\n",
    "    all_image_features, all_text_features = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            images, texts = batch\n",
    "            images = images.to(device=device, non_blocking=True)\n",
    "            texts = texts.to(device=device, non_blocking=True)\n",
    "\n",
    "            with autocast():\n",
    "                image_features, text_features, logit_scale = model(images, texts)\n",
    "                # features are accumulated in CPU tensors, otherwise GPU memory exhausted quickly\n",
    "                # however, system RAM is easily exceeded and compute time becomes problematic\n",
    "                all_image_features.append(image_features.cpu())\n",
    "                all_text_features.append(text_features.cpu())\n",
    "                logit_scale = logit_scale.mean()\n",
    "                logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "                logits_per_text = logits_per_image.t()\n",
    "\n",
    "                batch_size = images.shape[0]\n",
    "                labels = torch.arange(batch_size, device=device).long()\n",
    "                total_loss = (\n",
    "                    F.cross_entropy(logits_per_image, labels) +\n",
    "                    F.cross_entropy(logits_per_text, labels)\n",
    "                ) / 2\n",
    "\n",
    "            cumulative_loss += total_loss * batch_size\n",
    "            num_samples += batch_size\n",
    "            if is_master(args) and (i % 100) == 0:\n",
    "                logging.info(\n",
    "                    f\"Eval Epoch: {epoch} [{num_samples} / {samples_per_val}]\\t\"\n",
    "                    f\"Loss: {cumulative_loss / num_samples:.6f}\\t\")\n",
    "\n",
    "        val_metrics = get_metrics(\n",
    "            image_features=torch.cat(all_image_features),\n",
    "            text_features=torch.cat(all_text_features),\n",
    "            logit_scale=logit_scale.cpu(),\n",
    "        )\n",
    "        loss = cumulative_loss / num_samples\n",
    "        metrics.update(\n",
    "            {**val_metrics, \"val_loss\": loss.item(), \"epoch\": epoch, \"num_samples\": num_samples}\n",
    "        )\n",
    "        \n",
    "        # print(loss)\n",
    "\n",
    "    # if not metrics:\n",
    "    #     return metrics\n",
    "\n",
    "    logging.info(\n",
    "        f\"Eval Epoch: {epoch} \"\n",
    "        + \"\\t\".join([f\"{k}: {round(v, 4):.4f}\" for k, v in metrics.items()])\n",
    "    )\n",
    "\n",
    "    if args.save_logs:\n",
    "        for name, val in metrics.items():\n",
    "            if tb_writer is not None:\n",
    "                tb_writer.add_scalar(f\"val/{name}\", val, epoch)\n",
    "\n",
    "        with open(os.path.join(args.checkpoint_path, \"results.jsonl\"), \"a+\") as f:\n",
    "            f.write(json.dumps(metrics))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    if args.wandb:\n",
    "        assert wandb is not None, 'Please install wandb.'\n",
    "        for name, val in metrics.items():\n",
    "            wandb.log({f\"val/{name}\": val, 'epoch': epoch})\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_metrics(image_features, text_features, logit_scale):\n",
    "    metrics = {}\n",
    "    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()\n",
    "    logits_per_text = logits_per_image.t().detach().cpu()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "    ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        preds = torch.where(ranking == ground_truth)[1]\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 model loaded with best loss  tensor(2.2996, device='cuda:0')\n",
      "epoch  1 model loaded with best loss  tensor(2.2144, device='cuda:0')\n",
      "epoch  2 model loaded with best loss  tensor(2.1201, device='cuda:0')\n",
      "epoch  3 model loaded with best loss  tensor(2.0129, device='cuda:0')\n",
      "epoch  4 model loaded with best loss  tensor(1.8964, device='cuda:0')\n",
      "epoch  5 model loaded with best loss  tensor(1.7793, device='cuda:0')\n",
      "epoch  6 model loaded with best loss  tensor(1.6702, device='cuda:0')\n",
      "epoch  7 model loaded with best loss  tensor(1.5765, device='cuda:0')\n",
      "epoch  8 model loaded with best loss  tensor(1.4997, device='cuda:0')\n",
      "epoch  9 model loaded with best loss  tensor(1.4378, device='cuda:0')\n",
      "epoch  10 model loaded with best loss  tensor(1.3865, device='cuda:0')\n",
      "epoch  11 model loaded with best loss  tensor(1.3428, device='cuda:0')\n",
      "epoch  12 model loaded with best loss  tensor(1.3044, device='cuda:0')\n",
      "epoch  13 model loaded with best loss  tensor(1.2702, device='cuda:0')\n",
      "epoch  14 model loaded with best loss  tensor(1.2391, device='cuda:0')\n",
      "epoch  15 model loaded with best loss  tensor(1.2107, device='cuda:0')\n",
      "epoch  16 model loaded with best loss  tensor(1.1847, device='cuda:0')\n",
      "epoch  17 model loaded with best loss  tensor(1.1606, device='cuda:0')\n",
      "epoch  18 model loaded with best loss  tensor(1.1384, device='cuda:0')\n",
      "epoch  19 model loaded with best loss  tensor(1.1176, device='cuda:0')\n",
      "epoch  20 model loaded with best loss  tensor(1.0984, device='cuda:0')\n",
      "epoch  21 model loaded with best loss  tensor(1.0804, device='cuda:0')\n",
      "epoch  22 model loaded with best loss  tensor(1.0638, device='cuda:0')\n",
      "epoch  23 model loaded with best loss  tensor(1.0483, device='cuda:0')\n",
      "epoch  24 model loaded with best loss  tensor(1.0337, device='cuda:0')\n",
      "epoch  25 model loaded with best loss  tensor(1.0202, device='cuda:0')\n",
      "epoch  26 model loaded with best loss  tensor(1.0076, device='cuda:0')\n",
      "epoch  27 model loaded with best loss  tensor(0.9958, device='cuda:0')\n",
      "epoch  28 model loaded with best loss  tensor(0.9847, device='cuda:0')\n",
      "epoch  29 model loaded with best loss  tensor(0.9743, device='cuda:0')\n",
      "epoch  30 model loaded with best loss  tensor(0.9645, device='cuda:0')\n",
      "epoch  31 model loaded with best loss  tensor(0.9552, device='cuda:0')\n",
      "epoch  32 model loaded with best loss  tensor(0.9465, device='cuda:0')\n",
      "epoch  33 model loaded with best loss  tensor(0.9380, device='cuda:0')\n",
      "epoch  34 model loaded with best loss  tensor(0.9301, device='cuda:0')\n",
      "epoch  35 model loaded with best loss  tensor(0.9224, device='cuda:0')\n",
      "epoch  36 model loaded with best loss  tensor(0.9151, device='cuda:0')\n",
      "epoch  37 model loaded with best loss  tensor(0.9082, device='cuda:0')\n",
      "epoch  38 model loaded with best loss  tensor(0.9016, device='cuda:0')\n",
      "epoch  39 model loaded with best loss  tensor(0.8951, device='cuda:0')\n",
      "epoch  40 model loaded with best loss  tensor(0.8889, device='cuda:0')\n",
      "epoch  41 model loaded with best loss  tensor(0.8829, device='cuda:0')\n",
      "epoch  42 model loaded with best loss  tensor(0.8771, device='cuda:0')\n",
      "epoch  43 model loaded with best loss  tensor(0.8716, device='cuda:0')\n",
      "epoch  44 model loaded with best loss  tensor(0.8662, device='cuda:0')\n",
      "epoch  45 model loaded with best loss  tensor(0.8610, device='cuda:0')\n",
      "epoch  46 model loaded with best loss  tensor(0.8560, device='cuda:0')\n",
      "epoch  47 model loaded with best loss  tensor(0.8512, device='cuda:0')\n",
      "epoch  48 model loaded with best loss  tensor(0.8465, device='cuda:0')\n",
      "epoch  49 model loaded with best loss  tensor(0.8419, device='cuda:0')\n",
      "epoch  50 model loaded with best loss  tensor(0.8374, device='cuda:0')\n",
      "epoch  51 model loaded with best loss  tensor(0.8331, device='cuda:0')\n",
      "epoch  52 model loaded with best loss  tensor(0.8290, device='cuda:0')\n",
      "epoch  53 model loaded with best loss  tensor(0.8249, device='cuda:0')\n",
      "epoch  54 model loaded with best loss  tensor(0.8210, device='cuda:0')\n",
      "epoch  55 model loaded with best loss  tensor(0.8172, device='cuda:0')\n",
      "epoch  56 model loaded with best loss  tensor(0.8134, device='cuda:0')\n",
      "epoch  57 model loaded with best loss  tensor(0.8098, device='cuda:0')\n",
      "epoch  58 model loaded with best loss  tensor(0.8064, device='cuda:0')\n",
      "epoch  59 model loaded with best loss  tensor(0.8029, device='cuda:0')\n",
      "epoch  60 model loaded with best loss  tensor(0.7996, device='cuda:0')\n",
      "epoch  61 model loaded with best loss  tensor(0.7963, device='cuda:0')\n",
      "epoch  62 model loaded with best loss  tensor(0.7931, device='cuda:0')\n",
      "epoch  63 model loaded with best loss  tensor(0.7901, device='cuda:0')\n",
      "epoch  64 model loaded with best loss  tensor(0.7872, device='cuda:0')\n",
      "epoch  65 model loaded with best loss  tensor(0.7843, device='cuda:0')\n",
      "epoch  66 model loaded with best loss  tensor(0.7815, device='cuda:0')\n",
      "epoch  67 model loaded with best loss  tensor(0.7787, device='cuda:0')\n",
      "epoch  68 model loaded with best loss  tensor(0.7761, device='cuda:0')\n",
      "epoch  69 model loaded with best loss  tensor(0.7737, device='cuda:0')\n",
      "epoch  70 model loaded with best loss  tensor(0.7712, device='cuda:0')\n",
      "epoch  71 model loaded with best loss  tensor(0.7689, device='cuda:0')\n",
      "epoch  72 model loaded with best loss  tensor(0.7665, device='cuda:0')\n",
      "epoch  73 model loaded with best loss  tensor(0.7644, device='cuda:0')\n",
      "epoch  74 model loaded with best loss  tensor(0.7623, device='cuda:0')\n",
      "epoch  75 model loaded with best loss  tensor(0.7602, device='cuda:0')\n",
      "epoch  76 model loaded with best loss  tensor(0.7583, device='cuda:0')\n",
      "epoch  77 model loaded with best loss  tensor(0.7565, device='cuda:0')\n",
      "epoch  78 model loaded with best loss  tensor(0.7546, device='cuda:0')\n",
      "epoch  79 model loaded with best loss  tensor(0.7529, device='cuda:0')\n",
      "epoch  80 model loaded with best loss  tensor(0.7513, device='cuda:0')\n",
      "epoch  81 model loaded with best loss  tensor(0.7497, device='cuda:0')\n",
      "epoch  82 model loaded with best loss  tensor(0.7481, device='cuda:0')\n",
      "epoch  83 model loaded with best loss  tensor(0.7468, device='cuda:0')\n",
      "epoch  84 model loaded with best loss  tensor(0.7455, device='cuda:0')\n",
      "epoch  85 model loaded with best loss  tensor(0.7442, device='cuda:0')\n",
      "epoch  86 model loaded with best loss  tensor(0.7429, device='cuda:0')\n",
      "epoch  87 model loaded with best loss  tensor(0.7417, device='cuda:0')\n",
      "epoch  88 model loaded with best loss  tensor(0.7406, device='cuda:0')\n",
      "epoch  89 model loaded with best loss  tensor(0.7395, device='cuda:0')\n",
      "epoch  90 model loaded with best loss  tensor(0.7385, device='cuda:0')\n",
      "epoch  91 model loaded with best loss  tensor(0.7375, device='cuda:0')\n",
      "epoch  92 model loaded with best loss  tensor(0.7364, device='cuda:0')\n",
      "epoch  93 model loaded with best loss  tensor(0.7356, device='cuda:0')\n",
      "epoch  94 model loaded with best loss  tensor(0.7347, device='cuda:0')\n",
      "epoch  95 model loaded with best loss  tensor(0.7338, device='cuda:0')\n",
      "epoch  96 model loaded with best loss  tensor(0.7330, device='cuda:0')\n",
      "epoch  97 model loaded with best loss  tensor(0.7322, device='cuda:0')\n",
      "epoch  98 model loaded with best loss  tensor(0.7314, device='cuda:0')\n",
      "epoch  99 model loaded with best loss  tensor(0.7306, device='cuda:0')\n",
      "epoch  100 model loaded with best loss  tensor(0.7300, device='cuda:0')\n",
      "epoch  101 model loaded with best loss  tensor(0.7293, device='cuda:0')\n",
      "epoch  102 model loaded with best loss  tensor(0.7286, device='cuda:0')\n",
      "epoch  103 model loaded with best loss  tensor(0.7279, device='cuda:0')\n",
      "epoch  104 model loaded with best loss  tensor(0.7273, device='cuda:0')\n",
      "epoch  105 model loaded with best loss  tensor(0.7266, device='cuda:0')\n",
      "epoch  106 model loaded with best loss  tensor(0.7261, device='cuda:0')\n",
      "epoch  107 model loaded with best loss  tensor(0.7254, device='cuda:0')\n",
      "epoch  108 model loaded with best loss  tensor(0.7249, device='cuda:0')\n",
      "epoch  109 model loaded with best loss  tensor(0.7243, device='cuda:0')\n",
      "epoch  110 model loaded with best loss  tensor(0.7238, device='cuda:0')\n",
      "epoch  111 model loaded with best loss  tensor(0.7233, device='cuda:0')\n",
      "epoch  112 model loaded with best loss  tensor(0.7228, device='cuda:0')\n",
      "epoch  113 model loaded with best loss  tensor(0.7224, device='cuda:0')\n",
      "epoch  114 model loaded with best loss  tensor(0.7218, device='cuda:0')\n",
      "epoch  115 model loaded with best loss  tensor(0.7213, device='cuda:0')\n",
      "epoch  116 model loaded with best loss  tensor(0.7209, device='cuda:0')\n",
      "epoch  117 model loaded with best loss  tensor(0.7204, device='cuda:0')\n",
      "epoch  118 model loaded with best loss  tensor(0.7200, device='cuda:0')\n",
      "epoch  119 model loaded with best loss  tensor(0.7196, device='cuda:0')\n",
      "epoch  120 model loaded with best loss  tensor(0.7192, device='cuda:0')\n",
      "epoch  121 model loaded with best loss  tensor(0.7188, device='cuda:0')\n",
      "epoch  122 model loaded with best loss  tensor(0.7184, device='cuda:0')\n",
      "epoch  123 model loaded with best loss  tensor(0.7181, device='cuda:0')\n",
      "epoch  124 model loaded with best loss  tensor(0.7177, device='cuda:0')\n",
      "epoch  125 model loaded with best loss  tensor(0.7174, device='cuda:0')\n",
      "epoch  126 model loaded with best loss  tensor(0.7170, device='cuda:0')\n",
      "epoch  127 model loaded with best loss  tensor(0.7167, device='cuda:0')\n",
      "epoch  128 model loaded with best loss  tensor(0.7164, device='cuda:0')\n",
      "epoch  129 model loaded with best loss  tensor(0.7161, device='cuda:0')\n",
      "epoch  130 model loaded with best loss  tensor(0.7157, device='cuda:0')\n",
      "epoch  131 model loaded with best loss  tensor(0.7154, device='cuda:0')\n",
      "epoch  132 model loaded with best loss  tensor(0.7152, device='cuda:0')\n",
      "epoch  133 model loaded with best loss  tensor(0.7149, device='cuda:0')\n",
      "epoch  134 model loaded with best loss  tensor(0.7146, device='cuda:0')\n",
      "epoch  135 model loaded with best loss  tensor(0.7144, device='cuda:0')\n",
      "epoch  136 model loaded with best loss  tensor(0.7141, device='cuda:0')\n",
      "epoch  137 model loaded with best loss  tensor(0.7139, device='cuda:0')\n",
      "epoch  138 model loaded with best loss  tensor(0.7136, device='cuda:0')\n",
      "epoch  139 model loaded with best loss  tensor(0.7133, device='cuda:0')\n",
      "epoch  140 model loaded with best loss  tensor(0.7132, device='cuda:0')\n",
      "epoch  141 model loaded with best loss  tensor(0.7129, device='cuda:0')\n",
      "epoch  142 model loaded with best loss  tensor(0.7127, device='cuda:0')\n",
      "epoch  143 model loaded with best loss  tensor(0.7125, device='cuda:0')\n",
      "epoch  144 model loaded with best loss  tensor(0.7123, device='cuda:0')\n",
      "epoch  145 model loaded with best loss  tensor(0.7121, device='cuda:0')\n",
      "epoch  146 model loaded with best loss  tensor(0.7119, device='cuda:0')\n",
      "epoch  147 model loaded with best loss  tensor(0.7118, device='cuda:0')\n",
      "epoch  148 model loaded with best loss  tensor(0.7115, device='cuda:0')\n",
      "epoch  149 model loaded with best loss  tensor(0.7114, device='cuda:0')\n",
      "epoch  150 model loaded with best loss  tensor(0.7113, device='cuda:0')\n",
      "epoch  151 model loaded with best loss  tensor(0.7111, device='cuda:0')\n",
      "epoch  152 model loaded with best loss  tensor(0.7110, device='cuda:0')\n",
      "epoch  153 model loaded with best loss  tensor(0.7108, device='cuda:0')\n",
      "epoch  154 model loaded with best loss  tensor(0.7107, device='cuda:0')\n",
      "epoch  155 model loaded with best loss  tensor(0.7105, device='cuda:0')\n",
      "epoch  156 model loaded with best loss  tensor(0.7104, device='cuda:0')\n",
      "epoch  157 model loaded with best loss  tensor(0.7102, device='cuda:0')\n",
      "epoch  158 model loaded with best loss  tensor(0.7100, device='cuda:0')\n",
      "epoch  159 model loaded with best loss  tensor(0.7099, device='cuda:0')\n",
      "epoch  160 model loaded with best loss  tensor(0.7098, device='cuda:0')\n",
      "epoch  161 model loaded with best loss  tensor(0.7097, device='cuda:0')\n",
      "epoch  162 model loaded with best loss  tensor(0.7095, device='cuda:0')\n",
      "epoch  163 model loaded with best loss  tensor(0.7094, device='cuda:0')\n",
      "epoch  164 model loaded with best loss  tensor(0.7094, device='cuda:0')\n",
      "epoch  165 model loaded with best loss  tensor(0.7093, device='cuda:0')\n",
      "epoch  166 model loaded with best loss  tensor(0.7091, device='cuda:0')\n",
      "epoch  168 model loaded with best loss  tensor(0.7090, device='cuda:0')\n",
      "epoch  169 model loaded with best loss  tensor(0.7089, device='cuda:0')\n",
      "epoch  170 model loaded with best loss  tensor(0.7088, device='cuda:0')\n",
      "epoch  171 model loaded with best loss  tensor(0.7088, device='cuda:0')\n",
      "epoch  172 model loaded with best loss  tensor(0.7086, device='cuda:0')\n",
      "epoch  174 model loaded with best loss  tensor(0.7084, device='cuda:0')\n",
      "epoch  175 model loaded with best loss  tensor(0.7084, device='cuda:0')\n",
      "epoch  176 model loaded with best loss  tensor(0.7084, device='cuda:0')\n",
      "epoch  177 model loaded with best loss  tensor(0.7083, device='cuda:0')\n",
      "epoch  178 model loaded with best loss  tensor(0.7082, device='cuda:0')\n",
      "epoch  179 model loaded with best loss  tensor(0.7081, device='cuda:0')\n",
      "epoch  180 model loaded with best loss  tensor(0.7080, device='cuda:0')\n",
      "epoch  181 model loaded with best loss  tensor(0.7079, device='cuda:0')\n",
      "epoch  182 model loaded with best loss  tensor(0.7079, device='cuda:0')\n",
      "epoch  184 model loaded with best loss  tensor(0.7079, device='cuda:0')\n",
      "epoch  185 model loaded with best loss  tensor(0.7078, device='cuda:0')\n",
      "epoch  186 model loaded with best loss  tensor(0.7077, device='cuda:0')\n",
      "epoch  187 model loaded with best loss  tensor(0.7077, device='cuda:0')\n",
      "epoch  188 model loaded with best loss  tensor(0.7076, device='cuda:0')\n",
      "epoch  189 model loaded with best loss  tensor(0.7076, device='cuda:0')\n",
      "epoch  190 model loaded with best loss  tensor(0.7075, device='cuda:0')\n",
      "epoch  192 model loaded with best loss  tensor(0.7075, device='cuda:0')\n",
      "epoch  193 model loaded with best loss  tensor(0.7074, device='cuda:0')\n",
      "epoch  194 model loaded with best loss  tensor(0.7073, device='cuda:0')\n",
      "epoch  195 model loaded with best loss  tensor(0.7073, device='cuda:0')\n",
      "epoch  196 model loaded with best loss  tensor(0.7072, device='cuda:0')\n",
      "epoch  199 model loaded with best loss  tensor(0.7072, device='cuda:0')\n",
      "epoch  200 model loaded with best loss  tensor(0.7071, device='cuda:0')\n",
      "epoch  201 model loaded with best loss  tensor(0.7070, device='cuda:0')\n",
      "epoch  202 model loaded with best loss  tensor(0.7070, device='cuda:0')\n",
      "epoch  205 model loaded with best loss  tensor(0.7070, device='cuda:0')\n",
      "epoch  206 model loaded with best loss  tensor(0.7070, device='cuda:0')\n",
      "epoch  207 model loaded with best loss  tensor(0.7069, device='cuda:0')\n",
      "epoch  209 model loaded with best loss  tensor(0.7069, device='cuda:0')\n",
      "epoch  210 model loaded with best loss  tensor(0.7068, device='cuda:0')\n",
      "epoch  211 model loaded with best loss  tensor(0.7068, device='cuda:0')\n",
      "epoch  214 model loaded with best loss  tensor(0.7068, device='cuda:0')\n",
      "epoch  216 model loaded with best loss  tensor(0.7067, device='cuda:0')\n",
      "epoch  221 model loaded with best loss  tensor(0.7067, device='cuda:0')\n",
      "epoch  222 model loaded with best loss  tensor(0.7067, device='cuda:0')\n",
      "epoch  223 model loaded with best loss  tensor(0.7066, device='cuda:0')\n",
      "epoch  224 model loaded with best loss  tensor(0.7066, device='cuda:0')\n",
      "epoch  225 model loaded with best loss  tensor(0.7066, device='cuda:0')\n",
      "epoch  226 model loaded with best loss  tensor(0.7066, device='cuda:0')\n",
      "epoch  227 model loaded with best loss  tensor(0.7066, device='cuda:0')\n",
      "epoch  228 model loaded with best loss  tensor(0.7065, device='cuda:0')\n",
      "epoch  233 model loaded with best loss  tensor(0.7065, device='cuda:0')\n",
      "epoch  235 model loaded with best loss  tensor(0.7065, device='cuda:0')\n",
      "epoch  236 model loaded with best loss  tensor(0.7065, device='cuda:0')\n",
      "epoch  238 model loaded with best loss  tensor(0.7065, device='cuda:0')\n",
      "epoch  239 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  243 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  248 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  249 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  250 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  251 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  259 model loaded with best loss  tensor(0.7064, device='cuda:0')\n",
      "epoch  261 model loaded with best loss  tensor(0.7063, device='cuda:0')\n",
      "epoch  263 model loaded with best loss  tensor(0.7063, device='cuda:0')\n",
      "epoch  264 model loaded with best loss  tensor(0.7063, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "start_epoch = 0\n",
    "completed_epoch = 0\n",
    "\n",
    "best_val_loss = 1e6\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    # if is_master(args):\n",
    "    logging.info(f'Start epoch {epoch}')\n",
    "\n",
    "    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer)\n",
    "    val_loss = evaluate(model, data, completed_epoch, args, writer)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/metaclip_v4.pth')\n",
    "        print(\"epoch \", epoch, \"model loaded with best loss \", best_val_loss)\n",
    "    \n",
    "    completed_epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7063020467758179\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAHSCAYAAADIRU4IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAt0UlEQVR4nO3deZzddX3v8ffnLDNnTmbPTBayEEjCvsawC0IVCbRXWqsWtMoFLPVCLfZ6W62316V7r1Vbq2JRacSNegUKVBSXIoisAwRIiEDIQjbIJJlkMpl9zuf+cX6TDMMsZ+b8zvxmzu/1fDzGmfP7fc85H37+xPfju/3M3QUAAIDJSURdAAAAwExGmAIAACgCYQoAAKAIhCkAAIAiEKYAAACKQJgCAAAoQiqqL25qavIlS5ZE9fUAAAAFe/LJJ3e7e/NI5yILU0uWLFFLS0tUXw8AAFAwM9sy2jmG+QAAAIpAmAIAACgCYQoAAKAIhCkAAIAiEKYAAACKQJgCAAAoAmEKAACgCIQpAACAIhCmAAAAikCYAgAAKAJhCgAAoAiEKQAAgCIQpgAAAIpAmAIAACgCYQoAAKAIhCkAAIAilG2Ycnft7+pTd99A1KUAAIAyVrZh6rnt+3XqZ36ih17aHXUpAACgjJVtmGrIVkiS9nb2RlwJAAAoZ2UbpmZXB2HqIGEKAACUTtmGqap0UpWphNoIUwAAoITKNkyZmWbPqtAewhQAACihsg1TktQwq4KeKQAAUFJlHaYa6ZkCAAAlVvZhignoAACglMo+TDHMBwAASqm8w1S2Qgd6+tXTzy7oAACgNMo7TAV7Te3r7Iu4EgAAUK7KO0wFu6Dv6WCoDwAAlEZ5h6lZ+TDVxiNlAABAicQiTLE9AgAAKJVYhClW9AEAgFIp6zBVn62QGT1TAACgdMo6TCUTpvqqtPYe7Im6FAAAUKbKOkxJg8/nY2sEAABQGmUfpmbPqtAeeqYAAECJlH2YasjSMwUAAEqn7MPU7OoKJqADAICSKfsw1TirQm2dvcrlPOpSAABAGRo3TJnZIjO738yeN7N1ZnbjCG3eZ2bPmtlzZvawmZ1amnInriFboYGcq6O3P+pSAABAGUoV0KZf0kfd/Skzq5H0pJn91N2fH9Jmk6S3uHubmV0q6WZJZ5Wg3gmrq0pLkvYd7FNtJh1xNQAAoNyM2zPl7jvd/ang7wOS1ktaMKzNw+7eFrx8VNLCsAudrIYsz+cDAAClM6E5U2a2RNLpkh4bo9m1kn40yvuvM7MWM2tpbW2dyFdPWn026JnqYkUfAAAIX8FhysyqJd0u6SPu3j5Km4uUD1MfG+m8u9/s7ivdfWVzc/Nk6p2w+qBnah89UwAAoAQKmTMlM0srH6S+4+53jNLmFElfl3Spu+8Jr8TiNAz2THXSMwUAAMJXyGo+k/QNSevd/fOjtFks6Q5J73f3F8MtsTiDE9CZMwUAAEqhkJ6p8yS9X9JzZrYmOPYJSYslyd2/KumTkmZL+ko+e6nf3VeGXu0kpJIJ1WRS9EwBAICSGDdMuftDkmycNh+U9MGwigpbfTbNnCkAAFASZb8DupTfHoHVfAAAoBRiEabqqtJqY5gPAACUQCzCVEO2gmE+AABQEjEJU2kmoAMAgJKIRZiqy1aovbtPAzmPuhQAAFBmYhGmGrJpuUvtTEIHAAAhi0WYGnw+Hxt3AgCAsMUkTAXP56NnCgAAhCweYapq8Pl89EwBAIBwxSJMNQz2TLGiDwAAhCwWYerwnCnCFAAACFcswlRtJq2ESfsZ5gMAACGLRZhKJIxHygAAgJKIRZiS8iv62BoBAACELTZhqq4qrf1sjQAAAEIWqzDFDugAACBssQlTtVVptXf3R10GAAAoM/EJU5kUPVMAACB0sQlTg3Om3D3qUgAAQBmJTZiqrUqrP+fq6huIuhQAAFBG4hOmMvld0Nu7mDcFAADCE5swVRc87JjtEQAAQJhiE6Zqq1KSpPZuwhQAAAhPfMLUoWE+whQAAAhPbMIUw3wAAKAUYhOmaqvomQIAAOGLT5jKDM6ZYjUfAAAIT2zCVCqZ0KyKJMN8AAAgVLEJU1LwfD7CFAAACFGswtTgI2UAAADCEqswVZtJs88UAAAIVbzCVFWKx8kAAIBQxSxMMcwHAADCFa8wxTAfAAAIWbzCVFVaHT39yuU86lIAAECZiFWYqqtKy106wMadAAAgJLEKU4d3QWeoDwAAhCNeYYqHHQMAgJDFKkzV8bBjAAAQsliFqdpMEKYY5gMAACEZN0yZ2SIzu9/MnjezdWZ24whtzMy+aGYbzOxZM1tRmnKLU5dlmA8AAIQrVUCbfkkfdfenzKxG0pNm9lN3f35Im0slLQ9+zpJ0U/B7WqkJJqCzmg8AAIRl3J4pd9/p7k8Ffx+QtF7SgmHNLpd0q+c9KqnezOaHXm2RqitSMpPaCVMAACAkE5ozZWZLJJ0u6bFhpxZI2jrk9Ta9MXBFLpEwVVemmIAOAABCU3CYMrNqSbdL+oi7t0/my8zsOjNrMbOW1tbWyXxE0WozaYb5AABAaAoKU2aWVj5Ifcfd7xihyXZJi4a8Xhgcex13v9ndV7r7yubm5snUW7SaTEoHWM0HAABCUshqPpP0DUnr3f3zozS7W9IHglV9Z0va7+47Q6wzNDzsGAAAhKmQ1XznSXq/pOfMbE1w7BOSFkuSu39V0r2SLpO0QVKnpKtDrzQkNZmUXm3vjroMAABQJsYNU+7+kCQbp41LuiGsokqpJpPSS7uYMwUAAMIRqx3Qpfzz+RjmAwAAYYldmMpPQO9XvjMNAACgODEMU2kN5FxdfQNRlwIAAMpA7MLUoYcddzFvCgAAFC92Yerw8/mYNwUAAIoX2zDF8/kAAEAYYhemaquCYT56pgAAQAjiF6YODfPRMwUAAIoXuzBVE0xAZ84UAAAIQ+zCFKv5AABAmGIXpjLphFIJo2cKAACEInZhyswO7YIOAABQrNiFKSk/b4rVfAAAIAyxDFO1VfRMAQCAcMQyTNVUppkzBQAAQhHPMJVJsZoPAACEIpZhqraKnikAABCOWIYpVvMBAICwxDRMpXWgp18DOY+6FAAAMMPFMkwNPp+vo4feKQAAUJyYhimezwcAAMIRyzBVE/RMsaIPAAAUK5ZhqraKnikAABCOWIapwZ4pVvQBAIBixTRM5XumeD4fAAAoVizDVC09UwAAICSxDFM1rOYDAAAhiWWYqkglVJlK0DMFAACKFsswJeVX9DFnCgAAFCu2Yaomk1I7PVMAAKBIMQ5TaYb5AABA0WIbpmozKbV3McwHAACKE+MwlWY1HwAAKFpsw1RNJsUwHwAAKFpswxSr+QAAQBhiG6ZqKlPq7supbyAXdSkAAGAGi2+Y4pEyAAAgBLENU7VVwcOOWdEHAACKENswdfj5fPRMAQCAyYtxmBoc5qNnCgAATF5sw1Rt0DPFij4AAFCMccOUmd1iZrvMbO0o5+vM7B4ze8bM1pnZ1eGXGb7BnimezwcAAIpRSM/Uakmrxjh/g6Tn3f1USRdK+pyZVRRfWmnVMmcKAACEYNww5e4PSto7VhNJNWZmkqqDttM+oVQP9kyxmg8AABQhFcJnfEnS3ZJ2SKqR9HvuPu13wkwmTNWVPFIGAAAUJ4wJ6JdIWiPpCEmnSfqSmdWO1NDMrjOzFjNraW1tDeGri5N/Ph89UwAAYPLCCFNXS7rD8zZI2iTpuJEauvvN7r7S3Vc2NzeH8NXFqcmkWM0HAACKEkaYekXSWyXJzOZKOlbSxhA+t+RqM2mG+QAAQFHGnTNlZt9TfpVek5ltk/QpSWlJcvevSvorSavN7DlJJulj7r67ZBWHqCaT0u6O3qjLAAAAM9i4Ycrdrxzn/A5Jbw+toilUk0lr4+6DUZcBAABmsNjugC5JtVWs5gMAAMWJdZiqyaR1oLtP7h51KQAAYIaKeZhKqW/A1d037bfFAgAA01Ssw9ThR8qwPQIAAJicWIcpHnYMAACKFeswNdgzxcadAABgsuIdpqryPVOs6AMAAJMV6zBVw5wpAABQpJiHqWDOVBc9UwAAYHJiHaZYzQcAAIoV6zCVrUgqmTDmTAEAgEmLdZgyM1VXpuiZAgAAkxbrMCXlV/SxzxQAAJis2Iepmso0PVMAAGDSCFMZeqYAAMDkxT5M1Val1d5FzxQAAJic2IepmkyK1XwAAGDSYh+majPMmQIAAJNHmMqkdKCnX7mcR10KAACYgWIfpmoyablLB3sZ6gMAABNHmAqez8e8KQAAMBmxD1O1Vfnn87UzbwoAAExC7MMUPVMAAKAYhKlMvmeKFX0AAGAyYh+maoOeqfYueqYAAMDExT5M0TMFAACKQZga7JlizhQAAJiE2IepTDqpTDqh/TyfDwAATELsw5Qk1VdVqO1gb9RlAACAGYgwJak+m9Y+eqYAAMAkEKYUhKlOeqYAAMDEEaYkNWQr1NZJzxQAAJg4wpSk+myF9hGmAADAJBCmdHiYz92jLgUAAMwwhClJDdm0+nOujh72mgIAABNDmFJ+mE8SQ30AAGDCCFOS6qvyj5QhTAEAgIkiTElqmJXvmWpjewQAADBBhCnl50xJYuNOAAAwYYQpSXVVg3Om6JkCAAATQ5hSfmsESWo7SM8UAACYmHHDlJndYma7zGztGG0uNLM1ZrbOzB4It8TSSycTqqlMaV8XPVMAAGBiCumZWi1p1Wgnzaxe0lckvcPdT5T07lAqm2J12TSr+QAAwISNG6bc/UFJe8do8l5Jd7j7K0H7XSHVNqXyz+ejZwoAAExMGHOmjpHUYGa/MLMnzewDozU0s+vMrMXMWlpbW0P46vDU0zMFAAAmIYwwlZL0Jkm/KekSSf/HzI4ZqaG73+zuK919ZXNzcwhfHZ78w47pmQIAABOTCuEztkna4+4HJR00swclnSrpxRA+e8o0ZNNqo2cKAABMUBg9U3dJerOZpcwsK+ksSetD+NwpVV+VVnt3nwZyHnUpAABgBhm3Z8rMvifpQklNZrZN0qckpSXJ3b/q7uvN7MeSnpWUk/R1dx91G4Xpqj5bIXepvavv0ONlAAAAxjNumHL3Kwto81lJnw2loog0zAo27uzsJUwBAICCsQN6oCHLw44BAMDEEaYCTdWVkqTWA4QpAABQOMJUYDBM7e7oibgSAAAwkxCmArOr88N8hCkAADARhKlAOplQfTatPR0M8wEAgMIRpoZoqq6kZwoAAEwIYWqIpuoKwhQAAJgQwtQQs6srtZthPgAAMAGEqSGaqyu1+wA9UwAAoHCEqSGaqit0oKdf3X0DUZcCAABmCMLUEIN7Te05yFAfAAAoDGFqiEMbdzLUBwAACkSYGoKNOwEAwEQRpoY4NMzHij4AAFAgwtQQzTXBw47pmQIAAAUiTA2RSSdVXZlimA8AABSMMDVMfhd0hvkAAEBhCFPDzGbjTgAAMAGEqWGaqiu05yBhCgAAFIYwNcycmoxeaydMAQCAwhCmhplXl9H+rj519fJIGQAAMD7C1DDz6zKSpJ37uyKuBAAAzASEqWHm11VJkl7d3x1xJQAAYCYgTA1zuGeKMAUAAMZHmBpmHsN8AABgAghTw2TSSTVk0/RMAQCAghCmRjC/roo5UwAAoCCEqRHMr8vQMwUAAApCmBrBvLoMc6YAAEBBCFMjOKK+Sm2dferuY+NOAAAwNsLUCObV5lf0MW8KAACMhzA1gsG9pnYw1AcAAMZBmBrB4F5T9EwBAIDxEKZGMPhIGVb0AQCA8RCmRlBVkVR9Nq0d+xjmAwAAYyNMjWJxY1Zb2whTAABgbISpUSxqzGrr3s6oywAAANMcYWoUixuz2tbWqYGcR10KAACYxghTo1jcmFXfgLMTOgAAGBNhahSLG7OSpFcY6gMAAGMYN0yZ2S1mtsvM1o7T7gwz6zezd4VXXnQGwxTzpgAAwFgK6ZlaLWnVWA3MLCnpHyT9JISapoX5dRklE0bPFAAAGNO4YcrdH5S0d5xmH5Z0u6RdYRQ1HaSSCS2or9Ire5kzBQAARlf0nCkzWyDpdyTdVHw508vixiw9UwAAYExhTED/J0kfc/fceA3N7DozazGzltbW1hC+urQWNWb1yp6DUZcBAACmsVQIn7FS0m1mJklNki4zs353/4/hDd39Zkk3S9LKlSun/QZOixuzauvsU3t3n2oz6ajLAQAA01DRYcrdjxr828xWS/rPkYLUTHTk7MMr+k48oi7iagAAwHQ0bpgys+9JulBSk5ltk/QpSWlJcvevlrS6iA1uj7B5N2EKAACMbNww5e5XFvph7v7fi6pmmjm6eZYkaWNrR8SVAACA6Yod0MeQrUhpQX2VXiZMAQCAURCmxnF08yy93MqKPgAAMDLC1DiWNlfr5dYOuU/7xYcAACAChKlxLJ1Trc7eAb3a3h11KQAAYBoiTI1jaTAJ/eVdDPUBAIA3IkyNY1lztSQxCR0AAIyIMDWO5ppK1VSmCFMAAGBEhKlxmJmOnlNNmAIAACMiTBVgafMsbdhFmAIAAG9EmCrAsjnVeq29R/u7+qIuBQAATDOEqQIcP69WkvTiawcirgQAAEw3hKkCHDuvRpL0653tEVcCAACmG8JUAebXZVSbSWn9q/RMAQCA1yNMFcDMdNy8Wr1AmAIAAMMQpgp03PwavfDqAeVyPKMPAAAcRpgq0HHzatXR06/t+7qiLgUAAEwjhKkCHZqEzlAfAAAYgjBVIFb0AQCAkRCmClRdmdLixqzWv0qYAgAAhxGmJuDEI2q1djthCgAAHEaYmoCTFtTplb2d2t/JY2UAAEAeYWoCTllYJ0lau2N/xJUAAIDpgjA1AScdkQ9Tz20nTAEAgDzC1AQ0zKrQwoYqwhQAADiEMDVBJy+o01rCFAAACBCmJujkhXXasodJ6AAAII8wNUEnL2ASOgAAOIwwNUGDYeqZbfuiLQQAAEwLhKkJqs9W6OjmWXpqy76oSwEAANMAYWoSTl/UoDVb2+TuUZcCAAAiRpiahNMX12t3R6+27u2KuhQAABAxwtQkrFjcIEl6emtbxJUAAICoEaYm4Zi51cpWJPX0K/uiLgUAAESMMDUJqWRCpyys01Ov0DMFAEDcEaYmacXiBj2/o11dvQNRlwIAACJEmJqklUsa1J9zrdm6L+pSAABAhAhTk/SmIxtlJj2+aW/UpQAAgAgRpiapriqt4+bV6onNhCkAAOKMMFWEs45q1JNb2tQ3kIu6FAAAEBHCVBHOWNKorr4Brd3OQ48BAIgrwlQRzjgqv3knQ30AAMTXuGHKzG4xs11mtnaU8+8zs2fN7Dkze9jMTg2/zOlpTk1GRzfN0mMbCVMAAMRVIT1TqyWtGuP8JklvcfeTJf2VpJtDqGvGOOvo2Xp80171M28KAIBYGjdMufuDkkbtenH3h919cCvwRyUtDKm2GeG8ZbN1oKdfzzJvCgCAWAp7ztS1kn4U8mdOa+ccPVuS9PCG3RFXAgAAohBamDKzi5QPUx8bo811ZtZiZi2tra1hfXWkZldX6vj5tfrVhj1RlwIAACIQSpgys1MkfV3S5e4+aqpw95vdfaW7r2xubg7jq6eFNy+brSe3tPGcPgAAYqjoMGVmiyXdIen97v5i8SXNPOcua1LvQE4tW1jVBwBA3KTGa2Bm35N0oaQmM9sm6VOS0pLk7l+V9ElJsyV9xcwkqd/dV5aq4OnozCWNSidNv3xpt85fXj49bgAAYHzjhil3v3Kc8x+U9MHQKpqBZlWmdMaSRj3wQqs+cdnxUZcDAACmEDugh+QtxzTrhdcOaOf+rqhLAQAAU4gwFZILj50jSXrghfJYpQgAAApDmArJMXOrNa82owdeJEwBABAnhKmQmJkuPLZZD720W308WgYAgNggTIXowmPn6EBPv57YzBYJAADEBWEqRBcc06TKVEI/Wfda1KUAAIApQpgKUbYipQuOadZ9616Vu0ddDgAAmAKEqZCtOnGedu7v1rPb9kddCgAAmAKEqZC99fg5SiZMP173atSlAACAKUCYCll9tkLnHD1b9xGmAACIBcJUCVxy4lxtbD2oDbsORF0KAAAoMcJUCbz9xHmSpB+vpXcKAIByR5gqgbm1GZ2+uF73sUUCAABljzBVIqtOnKfntu/XtrbOqEsBAAAlRJgqkUuCoT56pwAAKG+EqRJZ0jRLJ8yv1d1rtkddCgAAKCHCVAm9c8UCPbNtP6v6AAAoY4SpEnrHaUcomTDd8RS9UwAAlCvCVAnNqcnoguVNuvPp7RrI8aw+AADKEWGqxN65YqF27u/Woxv3RF0KAAAoAcJUiV18wlzVZFK6/altUZcCAABKgDBVYpl0Ur91ynz9eO2rOtjTH3U5AAAgZISpKfDOFQvV2TvAw48BAChDhKkpsPLIBi1qrGKoDwCAMkSYmgJmpnetWKRfbdijTbsPRl0OAAAIEWFqilx55iKlEqZbH9kcdSkAACBEhKkpMqc2o8tOnq8ftGxjIjoAAGWEMDWFrjp3iQ709OsO5k4BAFA2CFNTaMXiep28oE7ffGSL3NkRHQCAckCYmkJmpqvOXaINuzr0qw3siA4AQDkgTE2x3zplvmbPqtDqhzdHXQoAAAgBYWqKZdJJXXnmYv3816/plT2dUZcDAACKRJiKwO+ffaRSCdMtv9oUdSkAAKBIhKkIzKvL6LdPW6DbnnhFezp6oi4HAAAUgTAVkT98y9Hq6c/pm8ydAgBgRiNMRWTZnBq9/YS5+uYjW9Te3Rd1OQAAYJIIUxH68G8s1/6uPn39l8ydAgBgpiJMReikBXW67OR5+sYvNzJ3CgCAGYowFbH/efEx6uob0E2/eDnqUgAAwCQQpiK2bE6NfnfFQt366Bbt3N8VdTkAAGCCxg1TZnaLme0ys7WjnDcz+6KZbTCzZ81sRfhllrcb37Zc7q4v/nxD1KUAAIAJKqRnarWkVWOcv1TS8uDnOkk3FV9WvCxsyOp9Zx2p77ds1abdB6MuBwAATMC4YcrdH5S0d4wml0u61fMelVRvZvPDKjAurr9oqSpTCf3ND9dHXQoAAJiAMOZMLZC0dcjrbcExTMCcmoz++K3L9bP1r+n+X++KuhwAAFCgKZ2AbmbXmVmLmbW0trZO5VfPCNecd5SObp6lT9+zTt19A1GXAwAAChBGmNouadGQ1wuDY2/g7je7+0p3X9nc3BzCV5eXilRCn3nHidqyp1Nf/+XGqMsBAAAFCCNM3S3pA8GqvrMl7Xf3nSF8biydv7xZl540T1+6f4O2tXVGXQ4AABhHIVsjfE/SI5KONbNtZnatmX3IzD4UNLlX0kZJGyR9TdL1Jas2Jv7it06QJP3lPc/L3SOuBgAAjCU1XgN3v3Kc8y7phtAqghbUV+kjbztGf/+jX+ueZ3fqHaceEXVJAABgFOyAPk198M1H6bRF9frkXWu160B31OUAAIBREKamqVQyoc+951R19Q7oE3c8x3AfAADTFGFqGlvaXK0/veRY/Wz9Lt3+1IgLJAEAQMQIU9PcNecdpTOXNOoz96xjdR8AANMQYWqaSyRM//juUyWX/ui7T6u3Pxd1SQAAYAjC1AyweHZWn333KVqzdZ/+9l6e3QcAwHRCmJohVp00X9ecd5RWP7xZP3yWPVEBAJguCFMzyMcvPU6nL67Xx25/Vpt2H4y6HAAAIMLUjFKRSuhL712hVNJ03a0tOtDdF3VJAADEHmFqhllQX6WvvG+FNu0+qA9/72kN5Nh/CgCAKBGmZqBzlzbpM5efqF+80MqEdAAAIjbus/kwPb3vrCP10msd+sZDm7R8TrWuOHNx1CUBABBLhKkZ7C9+83ht3H1Qf/EfazW3LqOLjp0TdUkAAMQOw3wzWCqZ0Jffe7qOnVej67/9lJ56pS3qkgAAiB3C1AxXk0lr9dVnak5tpa5Z/YReeu1A1CUBABArhKky0FxTqW9dc5bSyYQ+cMvjPMMPAIApRJgqE4tnZ3XrNWfqYE+/rrj5UW3dS6ACAGAqEKbKyPHza/XtD56l9q4+AhUAAFOEMFVmTllYr+/+wdnq6OnX7/3rI3plD4EKAIBSIkyVoZMW1Om7f3CWuvoG9O5/fVgvMikdAICSIUyVqROPqNNt150jd+k9//qI1mzdF3VJAACUJcJUGTt2Xo1+8KFzVZtJ671fe1QPvbQ76pIAACg7hKkyt3h2Vj/40Dla3JjVNauf0F1rtkddEgAAZYUwFQNzajP69+vO0emL63XjbWv0Lz9/Se4edVkAAJQFwlRM1GXTuvXaM/XO0xfocz99UX/2g2fV25+LuiwAAGY8HnQcI5WppD73nlO1qDGrf/75S9qyp1Nfet/pmlOTibo0AABmLHqmYsbM9CcXH6N/vuI0Pbt9n/7bvzykJ7fsjbosAABmLMJUTF1+2gLdef15yqSTuuLmR3XrI5uZRwUAwCQQpmLs+Pm1uvuGN+v85c365F3r9NHvP6Ou3oGoywIAYEYhTMVcXTatr39gpf7kbcfozjXbdfmXH9L6ne1RlwUAwIxBmIISCdONb1uu1VefqbbOPl3+pV/p67/cqFyOYT8AAMZDmMIhbzmmWT++8XxdcEyz/vqH63XVvz2u19q7oy4LAIBpjTCF15ldXamvfeBN+pvfOUlPbN6rVf/0oH703M6oywIAYNoiTOENzEzvO+tI/fCPz9fChqz+x3ee0vXfeVK7DtBLBQDAcIQpjGppc7XuuP5c/eklx+pn63fp4s8/qNuf3MYWCgAADEGYwpjSyYRuuGiZ7v3j87V8TrU++v+e0VX/9oS2tXVGXRoAANMCYQoFWTanWt//w3P0mXecqJbNe3Xx5x/Ul+/foJ5+9qUCAMQbYQoFSyRMV527RD/5kwt04bHN+ux9L+iSLzyo//r1a1GXBgBAZAhTmLCFDVnd9Ptv0reuPVPJhOma1S26dvUT2rz7YNSlAQAw5QhTmLTzlzfrRzdeoE9cdpwe3bhHF3/hAX3mnnXae7A36tIAAJgyhCkUpSKV0HUXLNX9/+tCvetNC/XNhzfrLf/3fn35/g085w8AEAsFhSkzW2VmL5jZBjP7+AjnF5vZ/Wb2tJk9a2aXhV8qprM5tRn93TtP0X0fuUBnHT1bn73vBV30j7/QbY+/or6BXNTlAQBQMjbenkFmlpT0oqSLJW2T9ISkK939+SFtbpb0tLvfZGYnSLrX3ZeM9bkrV670lpaWIsvHdPX4pr3623vXa83WfVrUWKUbLlymd65YqIoUnaEAgJnHzJ5095UjnSvk/9nOlLTB3Te6e6+k2yRdPqyNS6oN/q6TtGOyxaI8nHlUo+68/lx946qVashW6ON3PKeL/vEX+s5jW9hOAQBQVgoJUwskbR3yeltwbKhPS/p9M9sm6V5JHw6lOsxoZqa3Hj9Xd91wnv7t6jPUXFOp/33nWl302V/oW49sVncfoQoAMPOFNeZypaTV7r5Q0mWSvmVmb/hsM7vOzFrMrKW1tTWkr8Z0Z2a66Ng5uvP6c/Wta8/UEfVV+j93rdP5wUT1/Z19UZcIAMCkFTJn6hxJn3b3S4LXfy5J7v53Q9qsk7TK3bcGrzdKOtvdd432ucyZii931yMv79FND7ysX760W9mKpN6zcpGuOneJjmqaFXV5AAC8wVhzplIFvP8JScvN7ChJ2yVdIem9w9q8Iumtklab2fGSMpLoesKIzEznLmvSucuatH5nu772y4369qNbtPrhzTp/eZPef/aReuvxc5VMWNSlAgAwrnF7piQp2OrgnyQlJd3i7n9jZn8pqcXd7w5W8H1NUrXyk9H/zN1/MtZn0jOFoXYd6Na/P75V3338Fe3c360F9VV671mL9Z6Vi9RcUxl1eQCAmBurZ6qgMFUKhCmMpH8gp5+t36VvP7pFD23YrXTStOqk+XrXmxbqzcua6K0CAESi2GE+YMqkkgmtOmmeVp00Ty+3dujbj27RHU9t1z3P7NCcmkr99ukL9LsrFurYeTVRlwoAgCR6pjAD9PQP6P5f79LtT23X/b/epf6c64T5tfrNU+br0pPm6ejm6qhLBACUOYb5UDb2dPTonmd26D/W7NCarfskScfNq9GlJ83XZSfP0/K59FgBAMJHmEJZ2rGvS/ete1U/eu5VPbFlr9ylpc2zdMmJ83ThsXN0+uJ6pZM8vgYAUDzCFMrervZu3ff8a/rRczv12Ka9Gsi5aipTOnfZbL3lmDm64JgmLWzIRl0mAGCGIkwhVtq7+/Twhj164MVWPfDCLu3Y3y1JWjanWucuna0zljTqjCWNmleXibhSAMBMQZhCbLm7Xm7t0C9eaNUDL7bqyS1t6uzNPxNwYUOVzljSqJVLGnTGkkYtba5m6wUAwIgIU0CgfyCn9TsP6PHNe9Wyea+e2Nym3R09kqRsRVLHzavRCUfU6oT5dTrhiFodN69GmXQy4qoBAFEjTAGjcHdt2dOpli1tWrdjv9btaNf6He060NMvSUqYtLS5OghWtVraPEtL51RrcWOWye0AECNs2gmMwsy0pGmWljTN0rvetFBSPmBta+vSuh3ten5nu57f0a4nNu3VXWt2HHpfKmFa3JjV0c3V+YDVXK2jm2dpUWNWzdWVSjBcCACxQZgChjEzLWrMalFjVqtOmnfoeHt3nza2HtTG1g693Nqhja0H9XJrhx58sVW9A7lD7SqSCc2vz2hBfVX+p+Hw74X1Wc2ry6giRa8WAJQLwhRQoNpMWqctqtdpi+pfd3wg59re1qWXWzu0ra1T2/Z1aXtbl7bv69IDL7Zq14Ge17U3k+bWZLSgoUrN1ZVqqqnQ7FmVaqqpVHN1hZqqK/M/NZWaVZGUGb1cADCdEaaAIiUTpsWzs1o8e+R9rHr6B7RzX7e2ByFrMGzt2JcPYI9t6lFbZ9+I782kE4fDVXWlmgeDV3WFmmqGHK+uVG1ViuAFABEgTAElVplKHpqXNZq+gZz2HuxV64Ee7e7o0e6OXu3u6NGeIX9va+vUmq37tPdgj3IjrBupSCZUl02rrir/U5tJHf57yO/azODfh89XVxLEAGCyCFPANJBOJjS3NqO5teNvJDqQc7V15gPW7gPB744etXb0aH9nn/Z39am9u0+tHT3a0Nqh9q5+tXf3aayFuwnT4cCVyQetbEVKVemkshVJZYLfVemkqiryP4dfv75dVUVS2eB3ZSpBSANQ9ghTwAyTTNih4T3NG7+9JOVyrgM9/WrvCsJWELj2H3rdfyiEDZ7f09Grrr4BdfXmfzr7BjQwUpfYGMw0eiBLJ5WtSB0+PuTcoWA25L0VqYTSyYQqkglVpEwVyaTSKQteHz7HSkoAU40wBcRAImGHhvQWFfE5fQM5dfYOqLtvQJ1ByOrq61dXb06dvf2Hw1dw/lC7vgF19x7+u6t3QLs7etXZ26nuvsPv7e7LjV/EOFIJOxyuUokhYcuUTCSUSpiSCTv8O2lK2ODr4HzShrUb9r7kKMdHe09yjM86dD7xus9ImMlM+d/KB1NT/piC14NtBo+bJA15Pfy9Mh1qN/y9GvYdr3svvYvAmAhTAAqWTiZUV5VQXVW6JJ+fy7m6+w8Hte4hoax3IKe+gZx6+3PqHXD19h9+3TeQU8+Q14fODeTU2+/59/bnNOCugZyrP+cayOXUP+Dq6csFr/PHczlXfy43pN2Q3wNvPB4ng0HMzJQYGtA0chDTkPbD3ysNPfb69x76rmFBcfjnjBQyD4XEN4TMw+1f98804j+nFdBmhGMjtSzo+wr8rCkSVXYu5nvfdvxcXX3eUeEVM0GEKQDTRiJhylbk52vNBO6unOv14WsgCGU+9PVI4Swf5kY8HrzOf4eUc5e75MF3uiS55Mp/vwd/D7ZR0Gak9w5+5kjvHfxnGmw/9L1DP3OwfS74W0NrG/ZeDal5+Hs1WPOw+nM+7DMPvXfod4z83vwxH/W94/93Ouy13viekT5mxGPD3jtymxGO+6H/mHIj/fNOyfcW+bV9A8X3ahdjZvwbCwCmITNT0qRkguc3AnHGNswAAABFIEwBAAAUgTAFAABQBMIUAABAEQhTAAAARSBMAQAAFIEwBQAAUATCFAAAQBEIUwAAAEUgTAEAABSBMAUAAFAEwhQAAEARCFMAAABFIEwBAAAUgTAFAABQBMIUAABAEQhTAAAARSBMAQAAFMHcPZovNmuVtGUKvqpJ0u4p+J644HqGj2saPq5p+Lim4eOahq+U1/RId28e6URkYWqqmFmLu6+Muo5ywfUMH9c0fFzT8HFNw8c1DV9U15RhPgAAgCIQpgAAAIoQhzB1c9QFlBmuZ/i4puHjmoaPaxo+rmn4IrmmZT9nCgAAoJTi0DMFAABQMmUbpsxslZm9YGYbzOzjUdczU5nZZjN7zszWmFlLcKzRzH5qZi8FvxuirnM6M7NbzGyXma0dcmzEa2h5Xwzu22fNbEV0lU9fo1zTT5vZ9uBeXWNmlw059+fBNX3BzC6Jpurpy8wWmdn9Zva8ma0zsxuD49ynkzTGNeU+nSQzy5jZ42b2THBNPxMcP8rMHguu3b+bWUVwvDJ4vSE4v6RUtZVlmDKzpKQvS7pU0gmSrjSzE6Ktaka7yN1PG7Lc9OOSfu7uyyX9PHiN0a2WtGrYsdGu4aWSlgc/10m6aYpqnGlW643XVJK+ENyrp7n7vZIU/G//CkknBu/5SvDvCBzWL+mj7n6CpLMl3RBcN+7TyRvtmkrcp5PVI+k33P1USadJWmVmZ0v6B+Wv6TJJbZKuDdpfK6ktOP6FoF1JlGWYknSmpA3uvtHdeyXdJunyiGsqJ5dL+mbw9zcl/XZ0pUx/7v6gpL3DDo92DS+XdKvnPSqp3szmT0mhM8go13Q0l0u6zd173H2TpA3K/zsCAXff6e5PBX8fkLRe0gJxn07aGNd0NNyn4wjut47gZTr4cUm/IekHwfHh9+ng/fsDSW81MytFbeUaphZI2jrk9TaNfRNjdC7pJ2b2pJldFxyb6+47g79flTQ3mtJmtNGuIfducf4oGHa6ZcjwM9d0AoKhkNMlPSbu01AMu6YS9+mkmVnSzNZI2iXpp5JelrTP3fuDJkOv26FrGpzfL2l2Keoq1zCF8LzZ3Vco361/g5ldMPSk55eDsiS0CFzD0Nwkaany3f87JX0u0mpmIDOrlnS7pI+4e/vQc9ynkzPCNeU+LYK7D7j7aZIWKt9zd1y0FeWVa5jaLmnRkNcLg2OYIHffHvzeJelO5W/e1wa79IPfu6KrcMYa7Rpy706Su78W/Is2J+lrOjxEwjUtgJmllf8//e+4+x3BYe7TIox0TblPw+Hu+yTdL+kc5YeZU8Gpodft0DUNztdJ2lOKeso1TD0haXkww79C+Ul9d0dc04xjZrPMrGbwb0lvl7RW+Wt5VdDsKkl3RVPhjDbaNbxb0geC1VJnS9o/ZJgFYxg2Z+d3lL9Xpfw1vSJY2XOU8pOmH5/q+qazYB7JNyStd/fPDznFfTpJo11T7tPJM7NmM6sP/q6SdLHyc9Hul/SuoNnw+3Tw/n2XpP/yEm2umRq/yczj7v1m9keS7pOUlHSLu6+LuKyZaK6kO4P5eilJ33X3H5vZE5K+b2bXStoi6T0R1jjtmdn3JF0oqcnMtkn6lKS/18jX8F5Jlyk/+bRT0tVTXvAMMMo1vdDMTlN+KGqzpD+UJHdfZ2bfl/S88iusbnD3gQjKns7Ok/R+Sc8F81Ek6RPiPi3GaNf0Su7TSZsv6ZvBKseEpO+7+3+a2fOSbjOzv5b0tPIhVsHvb5nZBuUXrFxRqsLYAR0AAKAI5TrMBwAAMCUIUwAAAEUgTAEAABSBMAUAAFAEwhQAAEARCFMAAABFIEwBAAAUgTAFAABQhP8PXI4XW/XdTvoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_losses = [i.tolist() for i in val_losses]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(val_losses)\n",
    "print(min(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7063020467758179"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses[np.argmin(np.array(val_losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
