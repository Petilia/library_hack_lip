{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from open_clip import ClipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/docker_current/py_files/MLCLIP_exp/image_feat_array.npy', 'rb') as f:\n",
    "    image_feat_array = np.load(f)\n",
    "\n",
    "with open('/home/docker_current/py_files/MLCLIP_exp/text_feat_array.npy', 'rb') as f:\n",
    "    text_feat_array = np.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_feat_array, image_feat_array, \n",
    "                                                        test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train) \n",
    "y_train_t = torch.FloatTensor(y_train) \n",
    "X_val_t = torch.FloatTensor(X_test) \n",
    "y_val_t = torch.FloatTensor(y_test) \n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MetaCLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encode_image = ImageEncoder()\n",
    "        self.encode_text = TextEncoder()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        #open_clip realization\n",
    "        image_features = self.encode_image(image)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        text_features = self.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        return image_features, text_features, self.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from contextlib import suppress\n",
    "\n",
    "# amp_bfloat16 is more stable than amp float16 for clip training\n",
    "def get_autocast(precision):\n",
    "    if precision == 'amp':\n",
    "        return torch.cuda.amp.autocast\n",
    "    elif precision == 'amp_bfloat16':\n",
    "        return lambda: torch.cuda.amp.autocast(dtype=torch.bfloat16)\n",
    "    else:\n",
    "        return suppress\n",
    "\n",
    "def is_global_master(args):\n",
    "    return args.rank == 0\n",
    "\n",
    "def is_local_master(args):\n",
    "    return args.local_rank == 0\n",
    "\n",
    "def is_master(args, local=False):\n",
    "    return is_local_master(args) if local else is_global_master(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assign_learning_rate(optimizer, new_lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = new_lr\n",
    "\n",
    "def _warmup_lr(base_lr, warmup_length, step):\n",
    "    return base_lr * (step + 1) / warmup_length\n",
    "\n",
    "def cosine_lr(optimizer, base_lr, warmup_length, steps):\n",
    "    def _lr_adjuster(step):\n",
    "        if step < warmup_length:\n",
    "            lr = _warmup_lr(base_lr, warmup_length, step)\n",
    "        else:\n",
    "            e = step - warmup_length\n",
    "            es = steps - warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * base_lr\n",
    "        assign_learning_rate(optimizer, lr)\n",
    "        return lr\n",
    "    return _lr_adjuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import optim\n",
    "\n",
    "class Args:\n",
    "    device = 'cuda:0'\n",
    "    precision = 'amp'\n",
    "    local_loss = False\n",
    "    gather_with_grad = False\n",
    "    rank = 0\n",
    "    world_size = 1\n",
    "    horovod = False\n",
    "    norm_gradient_clip = None\n",
    "    batch_size = 64\n",
    "    wandb = False\n",
    "    val_frequency = 5\n",
    "    save_logs = False\n",
    "    epochs = 300\n",
    "    lr = 0.2e-4\n",
    "\n",
    "class SampleData():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataloader = DataLoader(dataset, batch_size=64)\n",
    "        self.dataloader.num_samples = len(dataset)\n",
    "        self.dataloader.num_batches = len(self.dataloader)\n",
    "\n",
    "args = Args()\n",
    "\n",
    "data = {}\n",
    "data['train'] = SampleData(train_dataset, args.batch_size)\n",
    "data['val'] = SampleData(val_dataset, args.batch_size)\n",
    "\n",
    "model = MetaCLIP()\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scaler = GradScaler() if args.precision == \"amp\" else None\n",
    "\n",
    "total_steps = data[\"train\"].dataloader.num_batches * args.epochs\n",
    "scheduler = cosine_lr(optimizer, args.lr, 15, total_steps)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaCLIP(\n",
       "  (encode_image): ImageEncoder(\n",
       "    (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "    (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "  )\n",
       "  (encode_text): TextEncoder(\n",
       "    (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "    (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "from open_clip import ClipLoss\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def unwrap_model(model):\n",
    "    if hasattr(model, 'module'):\n",
    "        return model.module\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, tb_writer=None):\n",
    "    device = torch.device(args.device)\n",
    "    autocast = get_autocast(args.precision)\n",
    "\n",
    "    model.train()\n",
    "    loss = ClipLoss(\n",
    "        local_loss=args.local_loss,\n",
    "        gather_with_grad=args.gather_with_grad,\n",
    "        cache_labels=True,\n",
    "        rank=args.rank,\n",
    "        world_size=args.world_size,\n",
    "        use_horovod=args.horovod)\n",
    "\n",
    "    # data['train'].set_epoch(epoch)  # set epoch in process safe manner via sampler or shared_epoch\n",
    "    dataloader = data['train'].dataloader\n",
    "    # dataloader = data['train']\n",
    "\n",
    "    num_batches_per_epoch = dataloader.num_batches\n",
    "    sample_digits = math.ceil(math.log(dataloader.num_samples + 1, 10))\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        step = num_batches_per_epoch * epoch + i\n",
    "        scheduler(step)\n",
    "\n",
    "        images, texts = batch\n",
    "        images = images.to(device=device, non_blocking=True)\n",
    "        texts = texts.to(device=device, non_blocking=True)\n",
    "\n",
    "        data_time_m.update(time.time() - end)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            image_features, text_features, logit_scale = model(images, texts)\n",
    "            total_loss = loss(image_features, text_features, logit_scale)\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(total_loss).backward()\n",
    "            if args.horovod:\n",
    "                optimizer.synchronize()\n",
    "                scaler.unscale_(optimizer)\n",
    "                if args.norm_gradient_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "                with optimizer.skip_synchronize():\n",
    "                    scaler.step(optimizer)\n",
    "            else:\n",
    "                if args.norm_gradient_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "                scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            if args.norm_gradient_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.norm_gradient_clip, norm_type=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Note: we clamp to 4.6052 = ln(100), as in the original paper.\n",
    "        with torch.no_grad():\n",
    "            unwrap_model(model).logit_scale.clamp_(0, math.log(100))\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        batch_count = i + 1\n",
    "\n",
    "        # print(total_loss)\n",
    "        \n",
    "        if is_master(args) and (i % 100 == 0 or batch_count == num_batches_per_epoch):\n",
    "            batch_size = len(images)\n",
    "            num_samples = batch_count * batch_size * args.world_size\n",
    "            samples_per_epoch = dataloader.num_samples\n",
    "            percent_complete = 100.0 * batch_count / num_batches_per_epoch\n",
    "\n",
    "            # NOTE loss is coarsely sampled, just master node and per log update\n",
    "            loss_m.update(total_loss.item(), batch_size)\n",
    "            logit_scale_scalar = logit_scale.item()\n",
    "            logging.info(\n",
    "                f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                f\"Batch (t): {batch_time_m.avg:.3f}, {args.batch_size*args.world_size / batch_time_m.val:#g}/s \"\n",
    "                f\"LR: {optimizer.param_groups[0]['lr']:5f} \"\n",
    "                f\"Logit Scale: {logit_scale_scalar:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Save train loss / etc. Using non avg meter values as loggers have their own smoothing\n",
    "            log_data = {\n",
    "                \"loss\": loss_m.val,\n",
    "                \"data_time\": data_time_m.val,\n",
    "                \"batch_time\": batch_time_m.val,\n",
    "                \"samples_per_scond\": args.batch_size*args.world_size / batch_time_m.val,\n",
    "                \"scale\":  logit_scale_scalar,\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "            }\n",
    "            for name, val in log_data.items():\n",
    "                name = \"train/\" + name\n",
    "                if tb_writer is not None:\n",
    "                    tb_writer.add_scalar(name, val, step)\n",
    "                if args.wandb:\n",
    "                    assert wandb is not None, 'Please install wandb.'\n",
    "                    wandb.log({name: val, 'step': step})\n",
    "\n",
    "            # resetting batch / data time meters per log window\n",
    "            batch_time_m.reset()\n",
    "            data_time_m.reset()\n",
    "    # end for\n",
    "\n",
    "\n",
    "def evaluate(model, data, epoch, args, tb_writer=None):\n",
    "    metrics = {}\n",
    "    # if not is_master(args):\n",
    "    #     return metrics\n",
    "    device = torch.device(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # zero_shot_metrics = zero_shot_eval(model, data, epoch, args)\n",
    "    # metrics.update(zero_shot_metrics)\n",
    "\n",
    "    autocast = get_autocast(args.precision)\n",
    "\n",
    "    \n",
    "    # if 'val' in data and (args.val_frequency and ((epoch % args.val_frequency) == 0 or epoch == args.epochs)):\n",
    "    dataloader = data['val'].dataloader\n",
    "    num_samples = 0\n",
    "    samples_per_val = dataloader.num_samples\n",
    "\n",
    "    # FIXME this does not scale past small eval datasets\n",
    "    # all_image_features @ all_text_features will blow up memory and compute very quickly\n",
    "    cumulative_loss = 0.0\n",
    "    all_image_features, all_text_features = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            images, texts = batch\n",
    "            images = images.to(device=device, non_blocking=True)\n",
    "            texts = texts.to(device=device, non_blocking=True)\n",
    "\n",
    "            with autocast():\n",
    "                image_features, text_features, logit_scale = model(images, texts)\n",
    "                # features are accumulated in CPU tensors, otherwise GPU memory exhausted quickly\n",
    "                # however, system RAM is easily exceeded and compute time becomes problematic\n",
    "                all_image_features.append(image_features.cpu())\n",
    "                all_text_features.append(text_features.cpu())\n",
    "                logit_scale = logit_scale.mean()\n",
    "                logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "                logits_per_text = logits_per_image.t()\n",
    "\n",
    "                batch_size = images.shape[0]\n",
    "                labels = torch.arange(batch_size, device=device).long()\n",
    "                total_loss = (\n",
    "                    F.cross_entropy(logits_per_image, labels) +\n",
    "                    F.cross_entropy(logits_per_text, labels)\n",
    "                ) / 2\n",
    "\n",
    "            cumulative_loss += total_loss * batch_size\n",
    "            num_samples += batch_size\n",
    "            if is_master(args) and (i % 100) == 0:\n",
    "                logging.info(\n",
    "                    f\"Eval Epoch: {epoch} [{num_samples} / {samples_per_val}]\\t\"\n",
    "                    f\"Loss: {cumulative_loss / num_samples:.6f}\\t\")\n",
    "\n",
    "        val_metrics = get_metrics(\n",
    "            image_features=torch.cat(all_image_features),\n",
    "            text_features=torch.cat(all_text_features),\n",
    "            logit_scale=logit_scale.cpu(),\n",
    "        )\n",
    "        loss = cumulative_loss / num_samples\n",
    "        metrics.update(\n",
    "            {**val_metrics, \"val_loss\": loss.item(), \"epoch\": epoch, \"num_samples\": num_samples}\n",
    "        )\n",
    "        \n",
    "        # print(loss)\n",
    "\n",
    "    # if not metrics:\n",
    "    #     return metrics\n",
    "\n",
    "    logging.info(\n",
    "        f\"Eval Epoch: {epoch} \"\n",
    "        + \"\\t\".join([f\"{k}: {round(v, 4):.4f}\" for k, v in metrics.items()])\n",
    "    )\n",
    "\n",
    "    if args.save_logs:\n",
    "        for name, val in metrics.items():\n",
    "            if tb_writer is not None:\n",
    "                tb_writer.add_scalar(f\"val/{name}\", val, epoch)\n",
    "\n",
    "        with open(os.path.join(args.checkpoint_path, \"results.jsonl\"), \"a+\") as f:\n",
    "            f.write(json.dumps(metrics))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    if args.wandb:\n",
    "        assert wandb is not None, 'Please install wandb.'\n",
    "        for name, val in metrics.items():\n",
    "            wandb.log({f\"val/{name}\": val, 'epoch': epoch})\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_metrics(image_features, text_features, logit_scale):\n",
    "    metrics = {}\n",
    "    logits_per_image = (logit_scale * image_features @ text_features.t()).detach().cpu()\n",
    "    logits_per_text = logits_per_image.t().detach().cpu()\n",
    "\n",
    "    logits = {\"image_to_text\": logits_per_image, \"text_to_image\": logits_per_text}\n",
    "    ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        preds = torch.where(ranking == ground_truth)[1]\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded with best loss  tensor(3.1427, device='cuda:0')\n",
      "model loaded with best loss  tensor(2.3618, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.8982, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.6564, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.5130, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.4113, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.3337, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.2719, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.2214, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.1800, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.1452, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.1165, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0923, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0721, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0552, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0409, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0286, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0182, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0090, device='cuda:0')\n",
      "model loaded with best loss  tensor(1.0007, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9934, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9868, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9808, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9754, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9703, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9658, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9615, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9577, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9538, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9504, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9471, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9440, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9412, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9384, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9357, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9333, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9309, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9287, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9266, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9247, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9227, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9210, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9192, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9177, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9162, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9147, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9133, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9120, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9108, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9097, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9086, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9076, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9068, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9058, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9049, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9042, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9034, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9026, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9020, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9013, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9007, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.9000, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8994, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8988, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8982, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8976, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8971, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8967, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8960, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8958, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8952, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8948, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8943, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8940, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8935, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8932, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8927, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8924, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8921, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8916, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8914, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8911, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8908, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8905, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8904, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8901, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8897, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8891, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8884, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8880, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8875, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8870, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8867, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8863, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8859, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8858, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8855, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8853, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8853, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8850, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8849, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8846, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8844, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8844, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8839, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8839, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8836, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8834, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8832, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8830, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8828, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8825, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8822, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8820, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8818, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8816, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8816, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8814, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8813, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8813, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8812, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8812, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8812, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8811, device='cuda:0')\n",
      "model loaded with best loss  tensor(0.8810, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "start_epoch = 0\n",
    "completed_epoch = 0\n",
    "\n",
    "best_val_loss = 1e6\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    # if is_master(args):\n",
    "    logging.info(f'Start epoch {epoch}')\n",
    "\n",
    "    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer)\n",
    "    val_loss = evaluate(model, data, completed_epoch, args, writer)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'metaclip_v3.pth')\n",
    "        print(\"model loaded with best loss \", best_val_loss)\n",
    "    \n",
    "    completed_epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.881014883518219\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_losses = [i.tolist() for i in val_losses]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(val_losses)\n",
    "print(min(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881014883518219"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses[np.argmin(np.array(val_losses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
