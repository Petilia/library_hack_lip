{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from open_clip import ClipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573, 768)\n"
     ]
    }
   ],
   "source": [
    "with open('/home/docker_current/py_files/MLCLIP_exp/image_feat_array.npy', 'rb') as f:\n",
    "    image_feat_array = np.load(f)\n",
    "\n",
    "with open('/home/docker_current/py_files/MLCLIP_exp/text_feat_array.npy', 'rb') as f:\n",
    "    text_feat_array = np.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_feat_array, image_feat_array, \n",
    "                                                        test_size=0.25, random_state=42)\n",
    "\n",
    "you_are_stupid = False\n",
    "\n",
    "if you_are_stupid:\n",
    "    X_train, y_train  = np.vstack((X_train, y_train)), np.vstack((y_train, X_train))\n",
    "    print(\"you are stupid\")\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train_t = torch.FloatTensor(X_train) \n",
    "y_train_t = torch.FloatTensor(y_train) \n",
    "X_val_t = torch.FloatTensor(X_test) \n",
    "y_val_t = torch.FloatTensor(y_test) \n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, ratio):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = self.fc2(x)\n",
    "        x = self.ratio * x + (1 - self.ratio) * input\n",
    "        return x\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, ratio):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        self.fc1 = nn.Linear(768, 1380)\n",
    "        self.fc2 = nn.Linear(1380, 768)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        x = self.fc2(x)\n",
    "        x = self.ratio * x + (1 - self.ratio) * input\n",
    "        return x\n",
    "\n",
    "class MetaCLIP(nn.Module):\n",
    "    def __init__(self, ratio=0.2):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.encode_image = ImageEncoder(self.ratio)\n",
    "        self.encode_text = TextEncoder(self.ratio)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        \n",
    "\n",
    "    def forward(self, image, text):\n",
    "        #open_clip realization\n",
    "        image_features = self.encode_image(image)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "        text_features = self.encode_text(text)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "        return image_features, text_features, self.logit_scale.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "from torch import optim\n",
    "from metaclip_train import cosine_lr\n",
    "\n",
    "class SampleData():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "        self.dataloader.num_samples = len(dataset)\n",
    "        self.dataloader.num_batches = len(self.dataloader)\n",
    "\n",
    "class Args:\n",
    "    device = 'cuda:0'\n",
    "    precision = 'amp'\n",
    "    local_loss = False\n",
    "    gather_with_grad = False\n",
    "    rank = 0\n",
    "    world_size = 1\n",
    "    horovod = False\n",
    "    norm_gradient_clip = None\n",
    "    batch_size = 64\n",
    "    wandb = False\n",
    "    val_frequency = 5\n",
    "    save_logs = False\n",
    "    epochs = 200\n",
    "    lr = 0.35e-4\n",
    "\n",
    "args = Args()\n",
    "\n",
    "data = {}\n",
    "data['train'] = SampleData(train_dataset, args.batch_size)\n",
    "data['val'] = SampleData(val_dataset, args.batch_size)\n",
    "\n",
    "model = MetaCLIP(ratio=0.5)\n",
    "model.to(args.device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "scaler = GradScaler() if args.precision == \"amp\" else None\n",
    "\n",
    "total_steps = data[\"train\"].dataloader.num_batches * args.epochs\n",
    "scheduler = cosine_lr(optimizer, args.lr, 15, total_steps)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MetaCLIP(\n",
       "   (encode_image): ImageEncoder(\n",
       "     (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "     (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "   )\n",
       "   (encode_text): TextEncoder(\n",
       "     (fc1): Linear(in_features=768, out_features=1380, bias=True)\n",
       "     (fc2): Linear(in_features=1380, out_features=768, bias=True)\n",
       "   )\n",
       " ),\n",
       " 0.5)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, model.ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 model loaded with best loss  tensor(2.0488, device='cuda:0')\n",
      "epoch  1 model loaded with best loss  tensor(1.6960, device='cuda:0')\n",
      "epoch  2 model loaded with best loss  tensor(1.4539, device='cuda:0')\n",
      "epoch  3 model loaded with best loss  tensor(1.2996, device='cuda:0')\n",
      "epoch  4 model loaded with best loss  tensor(1.1953, device='cuda:0')\n",
      "epoch  5 model loaded with best loss  tensor(1.1164, device='cuda:0')\n",
      "epoch  6 model loaded with best loss  tensor(1.0541, device='cuda:0')\n",
      "epoch  7 model loaded with best loss  tensor(1.0038, device='cuda:0')\n",
      "epoch  8 model loaded with best loss  tensor(0.9623, device='cuda:0')\n",
      "epoch  9 model loaded with best loss  tensor(0.9277, device='cuda:0')\n",
      "epoch  10 model loaded with best loss  tensor(0.8983, device='cuda:0')\n",
      "epoch  11 model loaded with best loss  tensor(0.8730, device='cuda:0')\n",
      "epoch  12 model loaded with best loss  tensor(0.8509, device='cuda:0')\n",
      "epoch  13 model loaded with best loss  tensor(0.8314, device='cuda:0')\n",
      "epoch  14 model loaded with best loss  tensor(0.8143, device='cuda:0')\n",
      "epoch  15 model loaded with best loss  tensor(0.7992, device='cuda:0')\n",
      "epoch  16 model loaded with best loss  tensor(0.7860, device='cuda:0')\n",
      "epoch  17 model loaded with best loss  tensor(0.7745, device='cuda:0')\n",
      "epoch  18 model loaded with best loss  tensor(0.7648, device='cuda:0')\n",
      "epoch  19 model loaded with best loss  tensor(0.7567, device='cuda:0')\n",
      "epoch  20 model loaded with best loss  tensor(0.7500, device='cuda:0')\n",
      "epoch  21 model loaded with best loss  tensor(0.7445, device='cuda:0')\n",
      "epoch  22 model loaded with best loss  tensor(0.7398, device='cuda:0')\n",
      "epoch  23 model loaded with best loss  tensor(0.7358, device='cuda:0')\n",
      "epoch  24 model loaded with best loss  tensor(0.7322, device='cuda:0')\n",
      "epoch  25 model loaded with best loss  tensor(0.7288, device='cuda:0')\n",
      "epoch  26 model loaded with best loss  tensor(0.7259, device='cuda:0')\n",
      "epoch  27 model loaded with best loss  tensor(0.7233, device='cuda:0')\n",
      "epoch  28 model loaded with best loss  tensor(0.7209, device='cuda:0')\n",
      "epoch  29 model loaded with best loss  tensor(0.7187, device='cuda:0')\n",
      "epoch  30 model loaded with best loss  tensor(0.7168, device='cuda:0')\n",
      "epoch  31 model loaded with best loss  tensor(0.7150, device='cuda:0')\n",
      "epoch  32 model loaded with best loss  tensor(0.7134, device='cuda:0')\n",
      "epoch  33 model loaded with best loss  tensor(0.7119, device='cuda:0')\n",
      "epoch  34 model loaded with best loss  tensor(0.7107, device='cuda:0')\n",
      "epoch  35 model loaded with best loss  tensor(0.7095, device='cuda:0')\n",
      "epoch  36 model loaded with best loss  tensor(0.7085, device='cuda:0')\n",
      "epoch  37 model loaded with best loss  tensor(0.7076, device='cuda:0')\n",
      "epoch  38 model loaded with best loss  tensor(0.7067, device='cuda:0')\n",
      "epoch  39 model loaded with best loss  tensor(0.7058, device='cuda:0')\n",
      "epoch  40 model loaded with best loss  tensor(0.7051, device='cuda:0')\n",
      "epoch  41 model loaded with best loss  tensor(0.7044, device='cuda:0')\n",
      "epoch  42 model loaded with best loss  tensor(0.7038, device='cuda:0')\n",
      "epoch  43 model loaded with best loss  tensor(0.7032, device='cuda:0')\n",
      "epoch  44 model loaded with best loss  tensor(0.7026, device='cuda:0')\n",
      "epoch  45 model loaded with best loss  tensor(0.7022, device='cuda:0')\n",
      "epoch  46 model loaded with best loss  tensor(0.7016, device='cuda:0')\n",
      "epoch  47 model loaded with best loss  tensor(0.7012, device='cuda:0')\n",
      "epoch  48 model loaded with best loss  tensor(0.7008, device='cuda:0')\n",
      "epoch  49 model loaded with best loss  tensor(0.7004, device='cuda:0')\n",
      "epoch  50 model loaded with best loss  tensor(0.7000, device='cuda:0')\n",
      "epoch  51 model loaded with best loss  tensor(0.6996, device='cuda:0')\n",
      "epoch  52 model loaded with best loss  tensor(0.6994, device='cuda:0')\n",
      "epoch  53 model loaded with best loss  tensor(0.6990, device='cuda:0')\n",
      "epoch  54 model loaded with best loss  tensor(0.6986, device='cuda:0')\n",
      "epoch  55 model loaded with best loss  tensor(0.6984, device='cuda:0')\n",
      "epoch  56 model loaded with best loss  tensor(0.6981, device='cuda:0')\n",
      "epoch  57 model loaded with best loss  tensor(0.6978, device='cuda:0')\n",
      "epoch  58 model loaded with best loss  tensor(0.6975, device='cuda:0')\n",
      "epoch  59 model loaded with best loss  tensor(0.6973, device='cuda:0')\n",
      "epoch  60 model loaded with best loss  tensor(0.6970, device='cuda:0')\n",
      "epoch  61 model loaded with best loss  tensor(0.6967, device='cuda:0')\n",
      "epoch  62 model loaded with best loss  tensor(0.6964, device='cuda:0')\n",
      "epoch  63 model loaded with best loss  tensor(0.6962, device='cuda:0')\n",
      "epoch  64 model loaded with best loss  tensor(0.6960, device='cuda:0')\n",
      "epoch  65 model loaded with best loss  tensor(0.6958, device='cuda:0')\n",
      "epoch  66 model loaded with best loss  tensor(0.6957, device='cuda:0')\n",
      "epoch  67 model loaded with best loss  tensor(0.6956, device='cuda:0')\n",
      "epoch  68 model loaded with best loss  tensor(0.6955, device='cuda:0')\n",
      "epoch  69 model loaded with best loss  tensor(0.6954, device='cuda:0')\n",
      "epoch  70 model loaded with best loss  tensor(0.6954, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "from metaclip_train import train_one_epoch, evaluate\n",
    "\n",
    "start_epoch = 0\n",
    "completed_epoch = 0\n",
    "\n",
    "best_val_loss = 1e6\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "    # if is_master(args):\n",
    "    # logging.info(f'Start epoch {epoch}')\n",
    "\n",
    "    train_one_epoch(model, data, epoch, optimizer, scaler, scheduler, args, writer)\n",
    "    val_loss = evaluate(model, data, completed_epoch, args, writer)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './weights/metaclip_v5_ratio_05.pth')\n",
    "        print(\"epoch \", epoch, \"model loaded with best loss \", best_val_loss)\n",
    "    \n",
    "    completed_epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6953872442245483\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_losses = [i.tolist() for i in val_losses]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(val_losses)\n",
    "print(min(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6953872442245483, 70)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses[np.argmin(np.array(val_losses))], np.argmin(np.array(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
